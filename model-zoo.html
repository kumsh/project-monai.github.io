<!DOCTYPE html>
<html lang="en" style="scroll-behavior: smooth;">
 <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QVGBCPFPJ6">
  </script>
  <script>
   window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'G-QVGBCPFPJ6');
  </script>
  <!-- Required meta tags -->
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
  <meta content="MONAI offers serveral frameworks, and we are adding to them all the time. Here, you’ll find the information you need to decide on what framework works best for your needs." name="description"/>
  <!--====== Favicon Icon ======-->
  <link href="assets/img/favicon.png" rel="shortcut icon" type="image/png"/>
  <title>
   MONAI Model Zoo
  </title>
  <!-- Tailwind css -->
  <link href="assets/css/tailwind.css" rel="stylesheet" type="text/css"/>
  <script src="https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/kutty@latest/dist/kutty.min.js">
  </script>
 </head>
 <body>
  <!-- Header Area wrapper Starts -->
  <header class="relative border-b border-neutral-neutral_gray2" id="header-wrap">
   <!-- Navbar Start -->
   <div class="navigation top-0 left-0 w-full z-30 duration-300">
    <div class="container">
     <nav class="navbar navbar-expand-lg flex justify-end items-center justify-between relative duration-300" x-data="{ one: false, two: false, three: false }">
      <a class="navbar-brand" href="index.html">
       <img alt="Logo" class="py-2" src="assets/img/MONAI-logo_color.png"/>
      </a>
      <button class="navbar-toggler focus:outline-none block lg:hidden" type="button">
       <svg class="w-5 h-5" fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
        <line x1="3" x2="21" y1="12" y2="12">
        </line>
        <line x1="3" x2="21" y1="6" y2="6">
        </line>
        <line x1="3" x2="21" y1="18" y2="18">
        </line>
       </svg>
      </button>
      <div class="collapse navbar-collapse hidden lg:block duration-300 shadow absolute top-100 left-0 mt-full bg-white z-20 px-5 w-full lg:static lg:bg-transparent lg:shadow-none">
       <ul class="navbar-nav mr-auto justify-end items-center lg:flex">
        <li class="nav-item relative">
         <a class="page-scroll" href="index.html">
          Home
         </a>
        </li>
        <li class="nav-item relative">
         <button @click="one = true" class="page-scroll">
          Frameworks
          <span class="chevron">
          </span>
         </button>
         <div @click.away="one = false" class="absolute w-52 top-0 z-50 w-screen max-w-xs p-2 mx-0 my-12 text-gray-800 border-2 border-brand-light transform bg-white rounded shadow lg:left-1/2 lg:-translate-x-1/2" x-cloak="" x-show.transition.in.opacity.out.opacity="one">
          <div class="flex flex-col space-y-1 font-medium text-gray-800">
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="label.html">
            MONAI Label
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="core.html">
            MONAI Core
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="deploy.html">
            MONAI Deploy
           </a>
          </div>
         </div>
        </li>
        <li class="nav-item relative">
         <button @click="two = true" class="page-scroll">
          Docs
          <span class="chevron">
          </span>
         </button>
         <div @click.away="two = false" class="absolute w-52 top-0 z-50 w-screen max-w-xs p-2 mx-0 my-12 text-gray-800 border-2 border-brand-light transform bg-white rounded shadow lg:left-1/2 lg:-translate-x-1/2" x-cloak="" x-show.transition.in.opacity.out.opacity="two">
          <div class="flex flex-col space-y-1 font-medium text-gray-800">
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="https://docs.monai.io/projects/label/en/latest/index.html" target="_blank">
            MONAI Label Docs
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="https://docs.monai.io/en/stable/" target="_blank">
            MONAI Core Docs
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="https://docs.monai.io/projects/monai-deploy-app-sdk/en/latest/" target="_blank">
            MONAI Deploy Docs
           </a>
          </div>
         </div>
        </li>
        <li class="nav-item relative">
         <button @click="three = true" class="page-scroll">
          Resources
          <span class="chevron">
          </span>
         </button>
         <div @click.away="three = false" class="absolute w-52 top-0 z-50 w-screen max-w-xs p-2 mx-0 my-12 text-gray-800 border-2 border-brand-light transform bg-white rounded shadow lg:left-1/2 lg:-translate-x-1/2" x-cloak="" x-show.transition.in.opacity.out.opacity="three">
          <div class="flex flex-col space-y-1 font-medium text-gray-800">
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="about.html">
            About Us
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="started.html">
            Getting Started
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="community.html">
            Community
           </a>
           <a class="px-3 py-2 transition rounded hover:bg-gray-200 hover:text-primary" href="https://medium.com/@monai" target="_blank">
            Blog
           </a>
          </div>
         </div>
        </li>
        <li class="nav-item relative">
         <a class="page-scroll active" href="model-zoo.html">
          Model Zoo
         </a>
        </li>
        <li class="nav-item relative">
         <a class="page-scroll" href="https://github.com/Project-MONAI" target="_blank">
          GitHub
         </a>
        </li>
       </ul>
      </div>
     </nav>
    </div>
   </div>
   <!-- Navbar End -->
  </header>
  <!-- Header Area wrapper End -->
  <section class="py-16" id="contributors">
   <div class="container">
    <div class="flex flex-wrap items-center">
     <div class="w-full lg:w-1/2">
      <h2 class="mb-6 section-heading">
       MONAI Model Zoo
      </h2>
      <div class="w-full sm:3/5">
       <p class="py-2">
        MONAI Model Zoo hosts a collection of medical imaging models in the MONAI Bundle format.
       </p>
       <p class="py-2">
        The
        <a class="text-brand" href="https://docs.monai.io/en/latest/bundle_intro.html" target="_blank">
         MONAI Bundle
        </a>
        format defines portable describes of deep learning models. A bundle includes the critical information necessary during a model development life cycle and allows users and programs to understand the purpose and usage of the models.
       </p>
      </div>
     </div>
     <div class="flex justify-center w-full lg:w-1/2">
      <img class="hidden lg:block lg:w-3/4 xl:auto" src="assets/img/community_header.png"/>
     </div>
    </div>
   </div>
  </section>
  <section class="py-8 bg-neutral-purewhite">
   <div class="container py-4 lg:py-16">
    <h2 class="text-brand-primary text-4xl font-bold">
     All Models
    </h2>
    <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-x-16 gap-y-16 py-8" id="all_models">
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Brats mri segmentation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Brats mri segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/brats_mri_segmentation_v0.3.9.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Myronenko, Andriy. '3D MRI brain tumor segmentation using autoencoder regularization.' International MICCAI Brainlesion Workshop. Springer, Cham, 2018. https://arxiv.org/abs/1810.11654
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           714
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           33.5MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.9
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data. The whole pipeline is modified from
            <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/med/models/clara_pt_brain_mri_segmentation">
             clara_pt_brain_mri_segmentation
            </a>
            .
           </p>
           <p>
            The model is trained to segment 3 nested subregions of primary brain tumors (gliomas): the "enhancing tumor" (ET), the "tumor core" (TC), the "whole tumor" (WT) based on 4 aligned input MRI scans (T1c, T1, T2, FLAIR).
- The ET is described by areas that show hyper intensity in T1c when compared to T1, but also when compared to "healthy" white matter in T1c.
- The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (fluid-filled) and the non-enhancing (solid) parts of the tumor.
-  The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edema (ED), which is typically depicted by hyper-intense signal in FLAIR.
           </p>
           <p>
            <img alt="Model workflow" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_brain_mri_segmentation_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training data is from the
            <a href="https://www.med.upenn.edu/cbica/sbia/brats2018/tasks.html">
             Multimodal Brain Tumor Segmentation Challenge (BraTS) 2018
            </a>
            .
           </p>
           <ul>
            <li>
             Target: 3 tumor subregions
            </li>
            <li>
             Task: Segmentation
            </li>
            <li>
             Modality: MRI
            </li>
            <li>
             Size: 285 3D volumes (4 channels each)
            </li>
           </ul>
           <p>
            The provided labelled data was partitioned, based on our own split, into training (200 studies), validation (42 studies) and testing (43 studies) datasets.
           </p>
           <h3>
            Preprocessing
           </h3>
           <p>
            The data list/split can be created with the script
            <code>
             scripts/prepare_datalist.py
            </code>
            .
           </p>
           <pre><code>python scripts/prepare_datalist.py --path your-brats18-dataset-path
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            This model utilized a similar approach described in 3D MRI brain tumor segmentation using autoencoder regularization, which was a winning method in BraTS2018 [1]. The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: At least 16GB of GPU memory.
            </li>
            <li>
             Actual Model Input: 224 x 224 x 144
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: DiceLoss
            </li>
           </ul>
           <h2>
            Input
           </h2>
           <p>
            4 channel aligned MRIs at 1 x 1 x 1 mm
- T1c
- T1
- T2
- FLAIR
           </p>
           <h2>
            Output
           </h2>
           <p>
            3 channels
- Label 0: TC tumor subregion
- Label 1: WT tumor subregion
- Label 2: ET tumor subregion
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Dice score was used for evaluating the performance of the model. This model achieved Dice scores on the validation data of:
- Tumor core (TC): 0.8559
- Whole tumor (WT): 0.9026
- Enhancing tumor (ET): 0.7905
- Average: 0.8518
           </p>
           <h4>
            Training Loss and Dice
           </h4>
           <p>
            <img alt="A graph showing the training loss and the mean dice over 300 epochs" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_brats_mri_segmentation_train.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            <img alt="A graph showing the validation mean dice over 300 epochs" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_brats_mri_segmentation_val.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=8 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] Myronenko, Andriy. "3D MRI brain tumor segmentation using autoencoder regularization." International MICCAI Brainlesion Workshop. Springer, Cham, 2018. https://arxiv.org/abs/1810.11654.
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/brats_mri_segmentation_v0.3.9.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Breast density classification
      </h3>
      <h5 class="text-brand-primary">
       Center for Augmented Intelligence in Imaging, Mayo Clinic Florida
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for classifying breast images (mammograms)
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Breast density classification
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/breast_density_classification_v0.1.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for classifying breast images (mammograms)
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Center for Augmented Intelligence in Imaging, Mayo Clinic Florida
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Gupta, Vikash, et al. A multi-reconstruction study of breast density estimation using Deep Learning. arXiv preprint arXiv:2202.08238 (2022).
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           247
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           94.5MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.1.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Description
           </h1>
           <p>
            A pre-trained model for breast-density classification.
           </p>
           <h1>
            Model Overview
           </h1>
           <p>
            This model is trained using transfer learning on InceptionV3. The model weights were fine tuned using the Mayo Clinic Data. The details of training and data is outlined in https://arxiv.org/abs/2202.08238.
           </p>
           <p>
            The bundle does not support torchscript.
           </p>
           <h1>
            Input and Output Formats
           </h1>
           <p>
            The input image should have the size [3, 299, 299]. The output is an array with probabilities for each of the four class.
           </p>
           <h1>
            Sample Data
           </h1>
           <p>
            In the folder
            <code>
             sample_data
            </code>
            few example input images are stored for each category of images. These images are stored in jpeg format for sharing purpose.
           </p>
           <h1>
            Input and Output Formats
           </h1>
           <p>
            The input image should have the size [299, 299, 3]. For a dicom image which are single channel. The channel can be repeated 3 times.
The output is an array with probabilities for each of the four class.
           </p>
           <h1>
            Commands Example
           </h1>
           <p>
            Create a json file with names of all the input files. Execute the following command
           </p>
           <pre><code>python scripts/create_dataset.py -base_dir &lt;path to the bundle root dir&gt;/sample_data -output_file configs/sample_image_data.json
</code></pre>
           <p>
            Change the
            <code>
             filename
            </code>
            for the field
            <code>
             data
            </code>
            with the absolute path for
            <code>
             sample_image_data.json
            </code>
           </p>
           <h1>
            Add scripts folder to your python path as follows
           </h1>
           <pre><code>export PYTHONPATH=$PYTHONPATH:&lt;path to the bundle root dir&gt;/scripts
</code></pre>
           <h1>
            Execute Inference
           </h1>
           <p>
            The inference can be executed as follows
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json configs/logging.conf
</code></pre>
           <h1>
            Execute training
           </h1>
           <p>
            It is a work in progress and will be shared in the next version soon.
           </p>
           <h1>
            Contributors
           </h1>
           <p>
            This model is made available from Center for Augmented Intelligence in Imaging, Mayo Clinic Florida. For questions email Vikash Gupta (gupta.vikash@mayo.edu).
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/breast_density_classification_v0.1.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Endoscopic inbody classification
      </h3>
      <h5 class="text-brand-primary">
       NVIDIA DLMED team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained binary classification model for endoscopic inbody classification task
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Endoscopic inbody classification
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/endoscopic_inbody_classification_v0.3.6.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained binary classification model for endoscopic inbody classification task
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           NVIDIA DLMED team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132-7141. https://arxiv.org/pdf/1709.01507.pdf
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           1179
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           184.7MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.6
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for the endoscopic inbody classification task and trained using the SEResNet50 structure, whose details can be found in [1]. All datasets are from private samples of
            <a href="https://www.activsurgical.com/">
             Activ Surgical
            </a>
            . Samples in training and validation dataset are from the same 4 videos, while test samples are from different two videos.
           </p>
           <p>
            The
            <a href="https://drive.google.com/file/d/14CS-s1uv2q6WedYQGeFbZeEWIkoyNa-x/view?usp=sharing">
             PyTorch model
            </a>
            and
            <a href="https://drive.google.com/file/d/1fOoJ4n5DWKHrt9QXTZ2sXwr9C-YvVGCM/view?usp=sharing">
             torchscript model
            </a>
            are shared in google drive. Modify the
            <code>
             bundle_root
            </code>
            parameter specified in
            <code>
             configs/train.json
            </code>
            and
            <code>
             configs/inference.json
            </code>
            to reflect where models are downloaded. Expected directory path to place downloaded models is
            <code>
             models/
            </code>
            under
            <code>
             bundle_root
            </code>
            .
           </p>
           <p>
            <img alt="image" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The datasets used in this work were provided by
            <a href="https://www.activsurgical.com/">
             Activ Surgical
            </a>
            .
           </p>
           <p>
            We've provided a
            <a href="https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/inbody_outbody_samples.zip">
             link
            </a>
            of 20 samples (10 in-body and 10 out-body) to show what this dataset looks like.
           </p>
           <h3>
            Preprocessing
           </h3>
           <p>
            After downloading this dataset, python script in
            <code>
             scripts
            </code>
            folder named
            <code>
             data_process
            </code>
            can be used to generate label json files by running the command below and modifying
            <code>
             datapath
            </code>
            to path of unziped downloaded data. Generated label json files will be stored in
            <code>
             label
            </code>
            folder under the bundle path.
           </p>
           <pre><code>python scripts/data_process.py --datapath /path/to/data/root
</code></pre>
           <p>
            By default, label path parameter in
            <code>
             train.json
            </code>
            and
            <code>
             inference.json
            </code>
            of this bundle is point to the generated
            <code>
             label
            </code>
            folder under bundle path. If you move these generated label files to another place, please modify the
            <code>
             train_json
            </code>
            ,
            <code>
             val_json
            </code>
            and
            <code>
             test_json
            </code>
            parameters specified in
            <code>
             configs/train.json
            </code>
            and
            <code>
             configs/inference.json
            </code>
            to where these label files are.
           </p>
           <p>
            The input label json should be a list made up by dicts which includes
            <code>
             image
            </code>
            and
            <code>
             label
            </code>
            keys. An example format is shown below.
           </p>
           <pre><code>[
    {
        "image":"/path/to/image/image_name0.jpg",
        "label": 0
    },
    {
        "image":"/path/to/image/image_name1.jpg",
        "label": 0
    },
    {
        "image":"/path/to/image/image_name2.jpg",
        "label": 1
    },
    ....
    {
        "image":"/path/to/image/image_namek.jpg",
        "label": 0
    },
]
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            The training as performed with the following:
- GPU: At least 12GB of GPU memory
- Actual Model Input: 256 x 256 x 3
- Optimizer: Adam
- Learning Rate: 1e-3
           </p>
           <h3>
            Input
           </h3>
           <p>
            A three channel video frame
           </p>
           <h3>
            Output
           </h3>
           <p>
            Two Channels
- Label 0: in body
- Label 1: out body
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Accuracy was used for evaluating the performance of the model. This model achieves an accuracy score of 0.98
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="A graph showing the training loss over 25 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_train_loss.png"/>
           </p>
           <h4>
            Validation Accuracy
           </h4>
           <p>
            <img alt="A graph showing the validation accuracy over 25 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_inbody_classification_val_accuracy.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training \
    --meta_file configs/metadata.json \
    --config_file configs/train.json \
    --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training \
    --meta_file configs/metadata.json \
    --config_file "['configs/train.json','configs/multi_gpu_train.json']" \
    --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <p>
            In addition, if using the 20 samples example dataset, the preprocessing script will divide the samples to 16 training samples, 2 validation samples and 2 test samples. However, pytorch multi-gpu training requires number of samples in dataloader larger than gpu numbers. Therefore, please use no more than 2 gpus to run this bundle if using the 20 samples example dataset.
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating \
    --meta_file configs/metadata.json \
    --config_file "['configs/train.json','configs/evaluate.json']" \
    --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating \
    --meta_file configs/metadata.json \
    --config_file configs/inference.json \
    --logging_file configs/logging.conf
</code></pre>
           <p>
            The classification result of every images in
            <code>
             test.json
            </code>
            will be printed to the screen.
           </p>
           <h4>
            Export checkpoint to TorchScript file:
           </h4>
           <pre><code>python -m monai.bundle ckpt_export network_def \
    --filepath models/model.ts \
    --ckpt_file models/model.pt \
    --meta_file configs/metadata.json \
    --config_file configs/inference.json
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7132-7141. https://arxiv.org/pdf/1709.01507.pdf
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/endoscopic_inbody_classification_v0.3.6.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Endoscopic tool segmentation
      </h3>
      <h5 class="text-brand-primary">
       NVIDIA DLMED team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained binary segmentation model for endoscopic tool segmentation
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Endoscopic tool segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/endoscopic_tool_segmentation_v0.4.1.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained binary segmentation model for endoscopic tool segmentation
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           NVIDIA DLMED team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. ICML, 2019a. https://arxiv.org/pdf/1905.11946.pdf
            </li>
            <li>
             O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015. https://arxiv.org/pdf/1505.04597.pdf
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           1091
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           81.7MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.4.1
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for the endoscopic tool segmentation task and is trained using a flexible unet structure with an efficient-b2 [1] as the backbone and a UNet architecture [2] as the decoder. Datasets use private samples from
            <a href="https://www.activsurgical.com/">
             Activ Surgical
            </a>
            .
           </p>
           <p>
            The
            <a href="https://drive.google.com/file/d/19yS3t2oLBiB7wT-qeQ82da95VJs_vzRK/view?usp=share_link">
             PyTorch model
            </a>
            and
            <a href="https://drive.google.com/file/d/1cDZ3Jr7mhpzdzaFyz8yHNowH8k0T1VZz/view?usp=share_link">
             torchscript model
            </a>
            are shared in google drive. Details can be found in large_files.yml file. Modify the "bundle_root" parameter specified in configs/train.json and configs/inference.json to reflect where models are downloaded. Expected directory path to place downloaded models is "models/" under "bundle_root".
           </p>
           <p>
            <img alt="image" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            Datasets used in this work were provided by
            <a href="https://www.activsurgical.com/">
             Activ Surgical
            </a>
            .
           </p>
           <p>
            Since datasets are private, existing public datasets like
            <a href="https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/Data/">
             EndoVis 2017
            </a>
            can be used to train a similar model.
           </p>
           <h3>
            Preprocessing
           </h3>
           <p>
            When using EndoVis or any other dataset, it should be divided into "train", "valid" and "test" folders. Samples in each folder would better be images and converted to jpg format. Otherwise, "images", "labels", "val_images" and "val_labels" parameters in "configs/train.json" and "datalist" in "configs/inference.json" should be modified to fit given dataset. After that, "dataset_dir" parameter in "configs/train.json" and "configs/inference.json" should be changed to root folder which contains previous "train", "valid" and "test" folders.
           </p>
           <p>
            Please notice that loading data operation in this bundle is adaptive. If images and labels are not in the same format, it may lead to a mismatching problem. For example, if images are in jpg format and labels are in npy format, PIL and Numpy readers will be used separately to load images and labels. Since these two readers have their own way to parse file's shape, loaded labels will be transpose of the correct ones and incur a missmatching problem.
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training as performed with the following:
- GPU: At least 12GB of GPU memory
- Actual Model Input: 736 x 480 x 3
- Optimizer: Adam
- Learning Rate: 1e-4
           </p>
           <h3>
            Input
           </h3>
           <p>
            A three channel video frame
           </p>
           <h3>
            Output
           </h3>
           <p>
            Two channels:
- Label 1: tools
- Label 0: everything else
           </p>
           <h2>
            Performance
           </h2>
           <p>
            IoU was used for evaluating the performance of the model. This model achieves a mean IoU score of 0.87.
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="A graph showing the training loss over 100 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_train_loss.png"/>
           </p>
           <h4>
            Validation IoU
           </h4>
           <p>
            <img alt="A graph showing the validation mean IoU over 100 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_endoscopic_tool_segmentation_val_iou.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config and
            <code>
             evaluate
            </code>
            config to execute multi-GPU evaluation:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']" --logging_file configs/logging.conf

#### Execute inference:

</code></pre>
           <p>
            python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
           </p>
           <pre><code>
#### Export checkpoint to TorchScript file:

</code></pre>
           <p>
            python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json
```
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Tan, M. and Le, Q. V. Efficientnet: Rethinking model scaling for convolutional neural networks. ICML, 2019a. https://arxiv.org/pdf/1905.11946.pdf
           </p>
           <p>
            [2] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015. https://arxiv.org/pdf/1505.04597.pdf
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/endoscopic_tool_segmentation_v0.4.1.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Lung nodule ct detection
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for volumetric (3D) detection of the lung lesion from CT image on LUNA16 dataset
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Lung nodule ct detection
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/lung_nodule_ct_detection_v0.5.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for volumetric (3D) detection of the lung lesion from CT image on LUNA16 dataset
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Lin, Tsung-Yi, et al. 'Focal loss for dense object detection. ICCV 2017
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           769
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           148.1MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.5.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for volumetric (3D) detection of the lung nodule from CT image.
           </p>
           <p>
            This model is trained on LUNA16 dataset (https://luna16.grand-challenge.org/Home/), using the RetinaNet (Lin, Tsung-Yi, et al. "Focal loss for dense object detection." ICCV 2017. https://arxiv.org/abs/1708.02002).
           </p>
           <p>
            <img alt="model workflow" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The dataset we are experimenting in this example is LUNA16 (https://luna16.grand-challenge.org/Home/), which is based on
            <a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI">
             LIDC-IDRI database
            </a>
            [3,4,5].
           </p>
           <p>
            LUNA16 is a public dataset of CT lung nodule detection. Using raw CT scans, the goal is to identify locations of possible nodules, and to assign a probability for being a nodule to each location.
           </p>
           <p>
            Disclaimer: We are not the host of the data. Please make sure to read the requirements and usage policies of the data and give credit to the authors of the dataset! We acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study.
           </p>
           <h3>
            10-fold data splitting
           </h3>
           <p>
            We follow the official 10-fold data splitting from LUNA16 challenge and generate data split json files using the script from
            <a href="https://github.com/MIC-DKFZ/nnDetection/blob/main/projects/Task016_Luna/scripts/prepare.py">
             nnDetection
            </a>
            .
           </p>
           <p>
            Please download the resulted json files from https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/LUNA16_datasplit-20220615T233840Z-001.zip.
           </p>
           <p>
            In these files, the values of "box" are the ground truth boxes in world coordinate.
           </p>
           <h3>
            Data resampling
           </h3>
           <p>
            The raw CT images in LUNA16 have various of voxel sizes. The first step is to resample them to the same voxel size.
In this model, we resampled them into 0.703125 x 0.703125 x 1.25 mm.
           </p>
           <p>
            Please following the instruction in Section 3.1 of https://github.com/Project-MONAI/tutorials/tree/main/detection to do the resampling.
           </p>
           <h3>
            Data download
           </h3>
           <p>
            The mhd/raw original data can be downloaded from
            <a href="https://luna16.grand-challenge.org/Home/">
             LUNA16
            </a>
            . The DICOM original data can be downloaded from
            <a href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI">
             LIDC-IDRI database
            </a>
            [3,4,5]. You will need to resample the original data to start training.
           </p>
           <p>
            Alternatively, we provide
            <a href="https://drive.google.com/drive/folders/1JozrufA1VIZWJIc5A1EMV3J4CNCYovKK?usp=share_link">
             resampled nifti images
            </a>
            and a copy of
            <a href="https://drive.google.com/drive/folders/1-enN4eNEnKmjltevKg3W2V-Aj0nriQWE?usp=share_link">
             original mhd/raw images
            </a>
            from
            <a href="https://luna16.grand-challenge.org/Home/">
             LUNA16
            </a>
            for users to download.
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: at least 16GB GPU memory
            </li>
            <li>
             Actual Model Input: 192 x 192 x 80
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-2
            </li>
            <li>
             Loss: BCE loss and L1 loss
            </li>
           </ul>
           <h3>
            Input
           </h3>
           <p>
            1 channel
- List of 3D CT patches
           </p>
           <h3>
            Output
           </h3>
           <p>
            In Training Mode: A dictionary of classification and box regression loss.
           </p>
           <p>
            In Evaluation Mode: A list of dictionaries of predicted box, classification label, and classification score.
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Coco metric is used for evaluating the performance of the model. The pre-trained model was trained and validated on data fold 0. This model achieves a mAP=0.853, mAR=0.994, AP(IoU=0.1)=0.862, AR(IoU=0.1)=1.0.
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="A graph showing the detection train loss" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_train_loss.png"/>
           </p>
           <h4>
            Validation Accuracy
           </h4>
           <p>
            The validation accuracy in this curve is the mean of mAP, mAR, AP(IoU=0.1), and AR(IoU=0.1) in Coco metric.
           </p>
           <p>
            <img alt="A graph showing the detection val accuracy" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_retinanet_detection_val_acc.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference on resampled LUNA16 images by setting
            <code>
             "whether_raw_luna16": false
            </code>
            in
            <code>
             inference.json
            </code>
            :
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <p>
            With the same command, we can execute inference on original LUNA16 images by setting
            <code>
             "whether_raw_luna16": true
            </code>
            in
            <code>
             inference.json
            </code>
            . Remember to also set
            <code>
             "data_list_file_path": "$@bundle_root + '/LUNA16_datasplit/mhd_original/dataset_fold0.json'"
            </code>
            and change
            <code>
             "dataset_dir"
            </code>
            .
           </p>
           <p>
            Note that in inference.json, the transform "LoadImaged" in "preprocessing" and "AffineBoxToWorldCoordinated" in "postprocessing" has
            <code>
             "affine_lps_to_ras": true
            </code>
            .
This depends on the input images. LUNA16 needs
            <code>
             "affine_lps_to_ras": true
            </code>
            .
It is possible that your inference dataset should set
            <code>
             "affine_lps_to_ras": false
            </code>
            .
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Lin, Tsung-Yi, et al. "Focal loss for dense object detection." ICCV 2017. https://arxiv.org/abs/1708.02002)
           </p>
           <p>
            [2] Baumgartner and Jaeger et al. "nnDetection: A self-configuring method for medical object detection." MICCAI 2021. https://arxiv.org/pdf/2106.00817.pdf
           </p>
           <p>
            [3] Armato III, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Van Beek, E. J. R., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C. , Qing, D. P. Y. , Roberts, R. Y., Smith, A. R., Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallam, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., Clarke, L. P. (2015). Data From LIDC-IDRI [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX
           </p>
           <p>
            [4] Armato SG 3rd, McLennan G, Bidaut L, McNitt-Gray MF, Meyer CR, Reeves AP, Zhao B, Aberle DR, Henschke CI, Hoffman EA, Kazerooni EA, MacMahon H, Van Beeke EJ, Yankelevitz D, Biancardi AM, Bland PH, Brown MS, Engelmann RM, Laderach GE, Max D, Pais RC, Qing DP, Roberts RY, Smith AR, Starkey A, Batrah P, Caligiuri P, Farooqi A, Gladish GW, Jude CM, Munden RF, Petkovska I, Quint LE, Schwartz LH, Sundaram B, Dodd LE, Fenimore C, Gur D, Petrick N, Freymann J, Kirby J, Hughes B, Casteele AV, Gupte S, Sallamm M, Heath MD, Kuhn MH, Dharaiya E, Burns R, Fryd DS, Salganicoff M, Anand V, Shreter U, Vastagh S, Croft BY.  The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans. Medical Physics, 38: 915--931, 2011. DOI: https://doi.org/10.1118/1.3528204
           </p>
           <p>
            [5] Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S., Phillips, S., Maffitt, D., Pringle, M., Tarbox, L., &amp; Prior, F. (2013). The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/lung_nodule_ct_detection_v0.5.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Mednist gan
      </h3>
      <h5 class="text-brand-primary">
       MONAI Team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       This example of a GAN generator produces hand xray images like those in the MedNIST dataset
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Mednist gan
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/mednist_gan_v0.4.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           This example of a GAN generator produces hand xray images like those in the MedNIST dataset
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI Team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           242
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           1.1MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.4.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            MedNIST GAN Hand Model
           </h1>
           <p>
            This model is a generator for creating images like the Hand category in the MedNIST dataset. It was trained as a GAN and accepts random values as inputs to produce an image output. The
            <code>
             train.json
            </code>
            file describes the training process along with the definition of the discriminator network used, and is based on the
            <a href="https://github.com/Project-MONAI/tutorials/blob/main/modules/mednist_GAN_workflow_dict.ipynb">
             MONAI GAN tutorials
            </a>
            .
           </p>
           <p>
            This is a demonstration network meant to just show the training process for this sort of network with MONAI, its outputs are not particularly good and are of the same tiny size as the images in MedNIST. The training process was very short so a network with a longer training time would produce better results.
           </p>
           <h3>
            Downloading the Dataset
           </h3>
           <p>
            Download the dataset from
            <a href="https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz">
             here
            </a>
            and extract the contents to a convenient location.
           </p>
           <p>
            The MedNIST dataset was gathered from several sets from
            <a href="https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions">
             TCIA
            </a>
            ,
            <a href="http://rsnachallenges.cloudapp.net/competitions/4">
             the RSNA Bone Age Challenge
            </a>
            ,
and
            <a href="https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest">
             the NIH Chest X-ray dataset
            </a>
            .
           </p>
           <p>
            The dataset is kindly made available by
            <a href="https://www.mayo.edu/research/labs/radiology-informatics/overview">
             Dr. Bradley J. Erickson M.D., Ph.D.
            </a>
            (Department of Radiology, Mayo Clinic)
under the Creative Commons
            <a href="https://creativecommons.org/licenses/by-sa/4.0/">
             CC BY-SA 4.0 license
            </a>
            .
           </p>
           <p>
            If you use the MedNIST dataset, please acknowledge the source.
           </p>
           <h3>
            Training
           </h3>
           <p>
            Assuming the current directory is the bundle directory, and the dataset was extracted to the directory
            <code>
             ./MedNIST
            </code>
            , the following command will train the network for 50 epochs:
           </p>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --bundle_root .
</code></pre>
           <p>
            Not also the output from the training will be placed in the
            <code>
             models
            </code>
            directory but will not overwrite the
            <code>
             model.pt
            </code>
            file that may be there already. You will have to manually rename the most recent checkpoint file to
            <code>
             model.pt
            </code>
            to use the inference script mentioned below after checking the results are correct. This saved checkpoint contains a dictionary with the generator weights stored as
            <code>
             model
            </code>
            and omits the discriminator.
           </p>
           <p>
            Another feature in the training file is the addition of sigmoid activation to the network by modifying it's structure at runtime. This is done with a line in the
            <code>
             training
            </code>
            section calling
            <code>
             add_module
            </code>
            on a layer of the network. This works best for training although the definition of the model now doesn't strictly match what it is in the
            <code>
             generator
            </code>
            section.
           </p>
           <p>
            The generator and discriminator networks were both trained with the
            <code>
             Adam
            </code>
            optimizer with a learning rate of 0.0002 and
            <code>
             betas
            </code>
            values
            <code>
             [0.5, 0.999]
            </code>
            . These have been emperically found to be good values for the optimizer and this GAN problem.
           </p>
           <h3>
            Inference
           </h3>
           <p>
            The included
            <code>
             inference.json
            </code>
            generates a set number of png samples from the network and saves these to the directory
            <code>
             ./outputs
            </code>
            . The output directory can be changed by setting the
            <code>
             output_dir
            </code>
            value, and the number of samples changed by setting the
            <code>
             num_samples
            </code>
            value. The following command line assumes it is invoked in the bundle directory:
           </p>
           <pre><code>python -m monai.bundle run inferring --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf --bundle_root .
</code></pre>
           <p>
            Note this script uses postprocessing to apply the sigmoid activation the model's outputs and to save the results to image files.
           </p>
           <h3>
            Export
           </h3>
           <p>
            The generator can be exported to a Torchscript bundle with the following:
           </p>
           <pre><code>python -m monai.bundle ckpt_export network_def --filepath mednist_gan.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.json
</code></pre>
           <p>
            The model can be loaded without MONAI code after this operation. For example, an image can be generated from a set of random values with:
           </p>
           <pre><code class="language-python">import torch
net = torch.jit.load("mednist_gan.ts")
latent = torch.rand(1, 64)
img = net(latent)  # (1,1,64,64)
</code></pre>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/mednist_gan_v0.4.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Mednist reg
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       This is an example of a ResNet and spatial transformer for hand xray image registration
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Mednist reg
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/mednist_reg_v0.0.4.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           This is an example of a ResNet and spatial transformer for hand xray image registration
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           105
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           40.3MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.0.4
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            MedNIST Hand Image Registration
           </h1>
           <p>
            Based on
            <a href="https://github.com/Project-MONAI/tutorials/tree/main/2d_registration">
             the tutorial of 2D registration
            </a>
           </p>
           <h2>
            Downloading the Dataset
           </h2>
           <p>
            Download the dataset
            <a href="https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz">
             from here
            </a>
            and extract the contents to a convenient location.
           </p>
           <p>
            The MedNIST dataset was gathered from several sets from
            <a href="https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions">
             TCIA
            </a>
            ,
            <a href="http://rsnachallenges.cloudapp.net/competitions/4">
             the RSNA Bone Age Challenge
            </a>
            ,
and
            <a href="https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest">
             the NIH Chest X-ray dataset
            </a>
            .
           </p>
           <p>
            The dataset is kindly made available by
            <a href="https://www.mayo.edu/research/labs/radiology-informatics/overview">
             Dr. Bradley J. Erickson M.D., Ph.D.
            </a>
            (Department of Radiology, Mayo Clinic)
under the Creative Commons
            <a href="https://creativecommons.org/licenses/by-sa/4.0/">
             CC BY-SA 4.0 license
            </a>
            .
           </p>
           <p>
            If you use the MedNIST dataset, please acknowledge the source.
           </p>
           <h2>
            Training
           </h2>
           <p>
            Training with same-subject image inputs
           </p>
           <pre><code class="language-bash">python -m monai.bundle run training --config_file configs/train.yaml --dataset_dir "/workspace/data/MedNIST/Hand"
</code></pre>
           <p>
            Training with cross-subject image inputs
           </p>
           <pre><code class="language-bash">python -m monai.bundle run training \
  --config_file configs/train.yaml \
  --dataset_dir "/workspace/data/MedNIST/Hand" \
  --cross_subjects True
</code></pre>
           <p>
            Training from an existing checkpoint file, for example,
            <code>
             models/model_key_metric=-0.0734.pt
            </code>
            :
           </p>
           <pre><code class="language-bash">python -m monai.bundle run training --config_file configs/train.yaml [...omitting other args] --ckpt "models/model_key_metric=-0.0734.pt"
</code></pre>
           <h2>
            Inference
           </h2>
           <p>
            The following figure shows an intra-subject (
            <code>
             --cross_subjects False
            </code>
            ) model inference results (Fixed, moving and predicted images from left to right)
           </p>
           <p>
            <img alt="fixed" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008502_fixed_6.png"/>
            <img alt="moving" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008502_moving_6.png"/>
            <img alt="predicted" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008502_pred_6.png"/>
           </p>
           <p>
            The command shows an inference workflow with the checkpoint
            <code>
             "models/model_key_metric=-0.0890.pt"
            </code>
            and using device
            <code>
             "cuda:1"
            </code>
            :
           </p>
           <pre><code class="language-bash">python -m monai.bundle run eval \
  --config_file configs/inference.yaml \
  --ckpt "models/model_key_metric=-0.0890.pt" \
  --logging_file configs/logging.conf \
  --device "cuda:1"
</code></pre>
           <h2>
            Fine-tuning for cross-subject alignments
           </h2>
           <p>
            The following commands starts a finetuning workflow based on the checkpoint
            <code>
             "models/model_key_metric=-0.0065.pt"
            </code>
            for
            <code>
             5
            </code>
            epochs using the global mutual information loss.
           </p>
           <pre><code class="language-bash">python -m monai.bundle run training \
  --config_file configs/train.yaml \
  --cross_subjects True \
  --ckpt "models/model_key_metric=-0.0065.pt" \
  --lr 0.000001 \
  --trainer#loss_function "@mutual_info_loss" \
  --max_epochs 5
</code></pre>
           <p>
            The following figure shows an inter-subject (
            <code>
             --cross_subjects True
            </code>
            ) model inference results (Fixed, moving and predicted images from left to right)
           </p>
           <p>
            <img alt="fixed" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008501_fixed_7.png"/>
            <img alt="moving" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008504_moving_7.png"/>
            <img alt="predicted" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/mednist_reg/docs/./examples/008504_pred_7.png"/>
           </p>
           <h2>
            Visualize the first pair of images for debugging (requires
            <code>
             matplotlib
            </code>
            )
           </h2>
           <pre><code class="language-bash">python -m monai.bundle run display --config_file configs/train.yaml
</code></pre>
           <pre><code class="language-bash">python -m monai.bundle run display --config_file configs/train.yaml --cross_subjects True
</code></pre>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/mednist_reg_v0.0.4.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Pancreas ct dints segmentation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       Searched architectures for volumetric (3D) segmentation of the pancreas from CT image
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Pancreas ct dints segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pancreas_ct_dints_segmentation_v0.3.6.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           Searched architectures for volumetric (3D) segmentation of the pancreas from CT image
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           371
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           943.3MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.6
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A neural architecture search algorithm for volumetric (3D) segmentation of the pancreas and pancreatic tumor from CT image. This model is trained using the neural network model from the neural architecture search algorithm, DiNTS [1].
           </p>
           <p>
            <img alt="image" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_workflow_4-1.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training dataset is the Panceas Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.
           </p>
           <ul>
            <li>
             Target: Liver and tumour
            </li>
            <li>
             Modality: Portal venous phase CT
            </li>
            <li>
             Size: 420 3D volumes (282 Training +139 Testing)
            </li>
            <li>
             Source: Memorial Sloan Kettering Cancer Center
            </li>
            <li>
             Challenge: Label unbalance with large (background), medium (pancreas) and small (tumour) structures.
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            The data list/split can be created with the script
            <code>
             scripts/prepare_datalist.py
            </code>
            .
           </p>
           <pre><code>python scripts/prepare_datalist.py --path /path-to-Task07_Pancreas/ --output configs/dataset_0.json
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with at least 16GB-memory GPUs.
           </p>
           <p>
            Actual Model Input: 96 x 96 x 96
           </p>
           <h3>
            Neural Architecture Search Configuration
           </h3>
           <p>
            The neural architecture search was performed with the following:
           </p>
           <ul>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: SGD
            </li>
            <li>
             Initial Learning Rate: 0.025
            </li>
            <li>
             Loss: DiceCELoss
            </li>
           </ul>
           <h3>
            Optimial Architecture Training Configuration
           </h3>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: SGD
            </li>
            <li>
             (Initial) Learning Rate: 0.025
            </li>
            <li>
             Loss: DiceCELoss
            </li>
            <li>
             Note: If out-of-memory or program crash occurs while caching the data set, please change the cache_rate in CacheDataset to a lower value in the range (0, 1).
            </li>
           </ul>
           <p>
            The segmentation of pancreas region is formulated as the voxel-wise 3-class classification. Each voxel is predicted as either foreground (pancreas body, tumour) or background. And the model is optimized with gradient descent method minimizing soft dice loss and cross-entropy loss between the predicted mask and ground truth segmentation.
           </p>
           <h3>
            Input
           </h3>
           <p>
            One channel
- CT image
           </p>
           <h3>
            Output
           </h3>
           <p>
            Three channels
- Label 2: pancreatic tumor
- Label 1: pancreas
- Label 0: everything else
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Dice score is used for evaluating the performance of the model. This model achieves a mean dice score of 0.62.
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            The loss over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)
           </p>
           <p>
            <img alt="Training loss over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_train_4-2.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            The mean dice score over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)
           </p>
           <p>
            <img alt="Validation mean dice score over 3200 epochs (the bright curve is smoothed, and the dark one is the actual curve)" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_validation_4-2.png"/>
           </p>
           <h3>
            Searched Architecture Visualization
           </h3>
           <p>
            Users can install Graphviz for visualization of searched architectures (needed in custom/decode_plot.py). The edges between nodes indicate global structure, and numbers next to edges represent different operations in the cell searching space. An example of searched architecture is shown as follows:
           </p>
           <p>
            <img alt="Example of Searched Architecture" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_net_arch_search_segmentation_searched_arch_example.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute model searching:
           </h4>
           <pre><code>python -m scripts.search run --config_file configs/search.yaml
</code></pre>
           <h4>
            Execute multi-GPU model searching (recommended):
           </h4>
           <pre><code>torchrun --nnodes=1 --nproc_per_node=8 -m scripts.search run --config_file configs/search.yaml
</code></pre>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.yaml --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.yaml','configs/multi_gpu_train.yaml']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.yaml','configs/evaluate.yaml']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.yaml --logging_file configs/logging.conf
</code></pre>
           <h4>
            Export checkpoint for TorchScript
           </h4>
           <pre><code>python -m monai.bundle ckpt_export network_def --filepath models/model.ts --ckpt_file models/model.pt --meta_file configs/metadata.json --config_file configs/inference.yaml
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pancreas_ct_dints_segmentation_v0.3.6.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Pathology nuclei classification
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for Nuclei Classification within Haematoxylin &amp; Eosin stained histology images
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Pathology nuclei classification
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclei_classification_v0.0.5.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for Nuclei Classification within Haematoxylin &amp; Eosin stained histology images
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images." Medical Image Analysis, Sept. 2019. https://doi.org/10.1016/j.media.2019.101563
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           697
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           50.8MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.0.5
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Description
           </h1>
           <p>
            A pre-trained model for classifying nuclei cells as the following types.
 - Other
 - Inflammatory
 - Epithelial
 - Spindle-Shaped
           </p>
           <h1>
            Model Overview
           </h1>
           <p>
            This model is trained using
            <a href="https://docs.monai.io/en/latest/networks.html#densenet121">
             DenseNet121
            </a>
            over
            <a href="https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet">
             ConSeP
            </a>
            dataset.
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training dataset is from https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet
           </p>
           <pre><code class="language-commandline">wget https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip
unzip -q consep_dataset.zip
</code></pre>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclei_classification/docs/images/dataset.jpeg"/>
            <br/>
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: at least 12GB of GPU memory
            </li>
            <li>
             Actual Model Input: 4 x 128 x 128
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: torch.nn.CrossEntropyLoss
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            After
            <a href="https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip">
             downloading this dataset
            </a>
            ,
python script
            <code>
             data_process.py
            </code>
            from
            <code>
             scripts
            </code>
            folder can be used to preprocess and generate the final dataset for training.
           </p>
           <pre><code class="language-commandline">python scripts/data_process.py --input /path/to/data/CoNSeP --output /path/to/data/CoNSePNuclei
</code></pre>
           <p>
            After generating the output files, please modify the
            <code>
             dataset_dir
            </code>
            parameter specified in
            <code>
             configs/train.json
            </code>
            and
            <code>
             configs/inference.json
            </code>
            to reflect the output folder which contains new dataset.json.
           </p>
           <p>
            Class values in dataset are
           </p>
           <ul>
            <li>
             1 = other
            </li>
            <li>
             2 = inflammatory
            </li>
            <li>
             3 = healthy epithelial
            </li>
            <li>
             4 = dysplastic/malignant epithelial
            </li>
            <li>
             5 = fibroblast
            </li>
            <li>
             6 = muscle
            </li>
            <li>
             7 = endothelial
            </li>
           </ul>
           <p>
            As part of pre-processing, the following steps are executed.
           </p>
           <ul>
            <li>
             Crop and Extract each nuclei Image + Label (128x128) based on the centroid given in the dataset.
            </li>
            <li>
             Combine classes 3 &amp; 4 into the epithelial class and 5,6 &amp; 7 into the spindle-shaped class.
            </li>
            <li>
             Update the label index for the target nuclie based on the class value
            </li>
            <li>
             Other cells which are part of the patch are modified to have label idex = 255
            </li>
           </ul>
           <p>
            Example
            <code>
             dataset.json
            </code>
            in output folder:
           </p>
           <pre><code class="language-json">{
  "training": [
    {
      "image": "/workspace/data/CoNSePNuclei/Train/Images/train_1_3_0001.png",
      "label": "/workspace/data/CoNSePNuclei/Train/Labels/train_1_3_0001.png",
      "nuclei_id": 1,
      "mask_value": 3,
      "centroid": [
        64,
        64
      ]
    }
  ],
  "validation": [
    {
      "image": "/workspace/data/CoNSePNuclei/Test/Images/test_1_3_0001.png",
      "label": "/workspace/data/CoNSePNuclei/Test/Labels/test_1_3_0001.png",
      "nuclei_id": 1,
      "mask_value": 3,
      "centroid": [
        64,
        64
      ]
    }
  ]
}
</code></pre>
           <h2>
            Input and output formats
           </h2>
           <h3>
            Input: 4 channels
           </h3>
           <ul>
            <li>
             3 RGB channels
            </li>
            <li>
             1 signal channel (label mask)
            </li>
           </ul>
           <h3>
            Output: 4 channels
           </h3>
           <ul>
            <li>
             0 = Other
            </li>
            <li>
             1 = Inflammatory
            </li>
            <li>
             2 = Epithelial
            </li>
            <li>
             3 = Spindle-Shaped
            </li>
           </ul>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclei_classification/docs/images/val_in_out.jpeg"/>
           </p>
           <h2>
            Scores
           </h2>
           <p>
            This model achieves the following F1 score on the validation data provided as part of the dataset:
           </p>
           <ul>
            <li>
             Train F1 score = 0.96
            </li>
            <li>
             Validation F1 score = 0.85
            </li>
           </ul>
           <hr/>
           <p>
            Confusion Metrics for
            <b>
             Validation
            </b>
            for individual classes are (at epoch 50):
           </p>
           <table>
            <thead>
             <tr>
              <th>
               Metric
              </th>
              <th>
               Other
              </th>
              <th>
               Inflammatory
              </th>
              <th>
               Epithelial
              </th>
              <th>
               Spindle-Shaped
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               Precision
              </td>
              <td>
               0.5846
              </td>
              <td>
               0.7143
              </td>
              <td>
               0.9158
              </td>
              <td>
               0.8399
              </td>
             </tr>
             <tr>
              <td>
               Recall
              </td>
              <td>
               0.2550
              </td>
              <td>
               0.8441
              </td>
              <td>
               0.9193
              </td>
              <td>
               0.8106
              </td>
             </tr>
             <tr>
              <td>
               F1-score
              </td>
              <td>
               0.3551
              </td>
              <td>
               0.7738
              </td>
              <td>
               0.9175
              </td>
              <td>
               0.8250
              </td>
             </tr>
            </tbody>
           </table>
           <hr/>
           <p>
            Confusion Metrics for
            <b>
             Training
            </b>
            for individual classes are (at epoch 50):
           </p>
           <table>
            <thead>
             <tr>
              <th>
               Metric
              </th>
              <th>
               Other
              </th>
              <th>
               Inflammatory
              </th>
              <th>
               Epithelial
              </th>
              <th>
               Spindle-Shaped
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               Precision
              </td>
              <td>
               0.9059
              </td>
              <td>
               0.9569
              </td>
              <td>
               0.9754
              </td>
              <td>
               0.9494
              </td>
             </tr>
             <tr>
              <td>
               Recall
              </td>
              <td>
               0.8370
              </td>
              <td>
               0.9547
              </td>
              <td>
               0.9790
              </td>
              <td>
               0.9502
              </td>
             </tr>
             <tr>
              <td>
               F1-score
              </td>
              <td>
               0.8701
              </td>
              <td>
               0.9558
              </td>
              <td>
               0.9772
              </td>
              <td>
               0.9498
              </td>
             </tr>
            </tbody>
           </table>
           <h2>
            Training Performance
           </h2>
           <p>
            A graph showing the training Loss and F1-score over 50 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclei_classification/docs/images/train_loss.jpeg"/>
            <br/>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclei_classification/docs/images/train_f1.jpeg"/>
            <br/>
           </p>
           <h2>
            Validation Performance
           </h2>
           <p>
            A graph showing the validation F1-score over 50 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclei_classification/docs/images/val_f1.jpeg"/>
            <br/>
           </p>
           <h2>
            commands example
           </h2>
           <p>
            Execute training:
           </p>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </p>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training related options depend on the actual running environment, thus you may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            or do some other necessary changes according to the machine you used.
Please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            for more details.
           </p>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config and
            <code>
             evaluate
            </code>
            config to execute multi-GPU evaluation:
           </p>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Execute inference:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            Disclaimer
           </h1>
           <p>
            This is an example, not to be used for diagnostic purposes.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images." Medical Image Analysis, Sept. 2019. [
            <a href="https://doi.org/10.1016/j.media.2019.101563">
             doi
            </a>
            ]
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclei_classification_v0.0.5.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Pathology nuclei segmentation classification
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A simultaneous segmentation and classification of nuclei within multitissue histology images based on CoNSeP data
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Pathology nuclei segmentation classification
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclei_segmentation_classification_v0.1.3.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A simultaneous segmentation and classification of nuclei within multitissue histology images based on CoNSeP data
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Simon Graham. 'HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images.' Medical Image Analysis, 2019. https://arxiv.org/abs/1812.06499
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           187
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           267.2MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.1.3
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for simultaneous segmentation and classification of nuclei within multi-tissue histology images based on CoNSeP data. The details of the model can be found in [1].
           </p>
           <h2>
            Workflow
           </h2>
           <p>
            The model is trained to simultaneous segment and classify nuclei. Training is done via a two-stage approach. First initialized the model with pre-trained weights on the
            <a href="https://ieeexplore.ieee.org/document/5206848">
             ImageNet dataset
            </a>
            , trained only the decoders for the first 50 epochs, and then fine-tuned all layers for another 50 epochs. There are two training modes in total. If "original" mode is specified, it uses [270, 270] and [80, 80] for
            <code>
             patch_size
            </code>
            and
            <code>
             out_size
            </code>
            respectively. If "fast" mode is specified, it uses [256, 256] and [164, 164] for
            <code>
             patch_size
            </code>
            and
            <code>
             out_size
            </code>
            respectively. The results we show below are based on the "fast" model.
           </p>
           <ul>
            <li>
             <p>
              We train the first stage with pre-trained weights from some internal data.
             </p>
            </li>
            <li>
             <p>
              The original author's repo also has pre-trained weights which is for non-commercial use. Each user is responsible for checking the content of models/datasets and the applicable licenses and determining if suitable for the intended use. The license for the pre-trained model is different than MONAI license. Please check the source where these weights are obtained from:
              <a href="https://github.com/vqdang/hover_net#data-format">
               https://github.com/vqdang/hover_net#data-format
              </a>
             </p>
            </li>
           </ul>
           <p>
            <code>
             PRETRAIN_MODEL_URL
            </code>
            is "https://drive.google.com/u/1/uc?id=1KntZge40tAHgyXmHYVqZZ5d2p_4Qr2l5&amp;export=download" which can be used in bash code below.
           </p>
           <p>
            <img alt="Model workflow" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_hovernet_pipeline.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training data is from
            <a href="https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/">
             https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/
            </a>
            .
           </p>
           <ul>
            <li>
             Target: segment instance-level nuclei and classify the nuclei type
            </li>
            <li>
             Task: Segmentation and classification
            </li>
            <li>
             Modality: RGB images
            </li>
            <li>
             Size: 41 image tiles (2009 patches)
            </li>
           </ul>
           <p>
            The provided labelled data was partitioned, based on the original split, into training (27 tiles) and testing (14 tiles) datasets.
           </p>
           <p>
            After download the datasets, please run
            <code>
             scripts/prepare_patches.py
            </code>
            to prepare patches from tiles. Prepared patches are saved in
            <code>
             your-concep-dataset-path
            </code>
            /Prepared. The implementation is referring to
            <a href="https://github.com/vqdang/hover_net/blob/master/extract_patches.py">
             https://github.com/vqdang/hover_net/blob/master/extract_patches.py
            </a>
            . The command is like:
           </p>
           <pre><code>python scripts/prepare_patches.py -root your-concep-dataset-path
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            This model utilized a two-stage approach. The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: At least 24GB of GPU memory.
            </li>
            <li>
             Actual Model Input: 256 x 256
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: HoVerNetLoss
            </li>
           </ul>
           <h2>
            Input
           </h2>
           <p>
            Input: RGB images
           </p>
           <h2>
            Output
           </h2>
           <p>
            Output: a dictionary with the following keys:
           </p>
           <ol>
            <li>
             nucleus_prediction: predict whether or not a pixel belongs to the nuclei or background
            </li>
            <li>
             horizontal_vertical: predict the horizontal and vertical distances of nuclear pixels to their centres of mass
            </li>
            <li>
             type_prediction: predict the type of nucleus for each pixel
            </li>
           </ol>
           <h2>
            Model Performance
           </h2>
           <p>
            The achieved metrics on the validation data are:
           </p>
           <p>
            Fast mode:
- Binary Dice: 0.8293
- PQ: 0.4936
- F1d: 0.7480
           </p>
           <h4>
            Training Loss and Dice
           </h4>
           <p>
            stage1:
            <img alt="A graph showing the training loss and the mean dice over 50 epochs in stage1" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclei_seg_cls_train_stage1_fast.png"/>
           </p>
           <p>
            stage2:
            <img alt="A graph showing the training loss and the mean dice over 50 epochs in stage2" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclei_seg_cls_train_stage2_fast.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            stage1:
           </p>
           <p>
            <img alt="A graph showing the validation mean dice over 50 epochs in stage1" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclei_seg_cls_val_stage1_fast.png"/>
           </p>
           <p>
            stage2:
           </p>
           <p>
            <img alt="A graph showing the validation mean dice over 50 epochs in stage2" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_nuclei_seg_cls_val_stage2_fast.png"/>
           </p>
           <h2>
            commands example
           </h2>
           <p>
            Execute training:
           </p>
           <ul>
            <li>
             Run first stage
            </li>
           </ul>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --network_def#pretrained_url `PRETRAIN_MODEL_URL` --stage 0
</code></pre>
           <ul>
            <li>
             Run second stage
            </li>
           </ul>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --network_def#freeze_encoder false --network_def#pretrained_url None --stage 1
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </p>
           <ul>
            <li>
             Run first stage
            </li>
           </ul>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf --train#dataloader#batch_size 8 --network_def#freeze_encoder true --network_def#pretrained_url `PRETRAIN_MODEL_URL` --stage 0
</code></pre>
           <ul>
            <li>
             Run second stage
            </li>
           </ul>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf --train#dataloader#batch_size 4 --network_def#freeze_encoder false --network_def#pretrained_url None --stage 1
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h3>
            Execute inference
           </h3>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            Disclaimer
           </h1>
           <p>
            This is an example, not to be used for diagnostic purposes.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, Nasir Rajpoot, Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images, Medical Image Analysis, 2019 https://doi.org/10.1016/j.media.2019.101563
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclei_segmentation_classification_v0.1.3.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Pathology nuclick annotation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for segmenting nuclei cells with user clicks/interactions
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Pathology nuclick annotation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclick_annotation_v0.0.5.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for segmenting nuclei cells with user clicks/interactions
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Koohbanani, Navid Alemi, et al. "NuClick: A Deep Learning Framework for Interactive Segmentation of Microscopy Images." https://arxiv.org/abs/2005.14511
            </li>
            <li>
             S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images." Medical Image Analysis, Sept. 2019. https://doi.org/10.1016/j.media.2019.101563
            </li>
            <li>
             NuClick PyTorch Implementation, https://github.com/mostafajahanifar/nuclick_torch
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           691
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           55.6MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.0.5
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Description
           </h1>
           <p>
            A pre-trained model for segmenting nuclei cells with user clicks/interactions.
           </p>
           <p>
            <img alt="nuclick" src="https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/11.gif"/>
            <img alt="nuclick" src="https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/33.gif"/>
            <img alt="nuclick" src="https://github.com/mostafajahanifar/nuclick_torch/raw/master/docs/22.gif"/>
           </p>
           <h1>
            Model Overview
           </h1>
           <p>
            This model is trained using
            <a href="https://docs.monai.io/en/latest/networks.html#basicunet">
             BasicUNet
            </a>
            over
            <a href="https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet">
             ConSeP
            </a>
            dataset.
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training dataset is from https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet
           </p>
           <pre><code class="language-commandline">wget https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip
unzip -q consep_dataset.zip
</code></pre>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclick_annotation/docs/images/dataset.jpeg"/>
            <br/>
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: at least 12GB of GPU memory
            </li>
            <li>
             Actual Model Input: 5 x 128 x 128
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: DiceLoss
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            After
            <a href="https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/consep_dataset.zip">
             downloading this dataset
            </a>
            ,
python script
            <code>
             data_process.py
            </code>
            from
            <code>
             scripts
            </code>
            folder can be used to preprocess and generate the final dataset for training.
           </p>
           <pre><code>python scripts/data_process.py --input /path/to/data/CoNSeP --output /path/to/data/CoNSePNuclei
</code></pre>
           <p>
            After generating the output files, please modify the
            <code>
             dataset_dir
            </code>
            parameter specified in
            <code>
             configs/train.json
            </code>
            and
            <code>
             configs/inference.json
            </code>
            to reflect the output folder which contains new dataset.json.
           </p>
           <p>
            Class values in dataset are
           </p>
           <ul>
            <li>
             1 = other
            </li>
            <li>
             2 = inflammatory
            </li>
            <li>
             3 = healthy epithelial
            </li>
            <li>
             4 = dysplastic/malignant epithelial
            </li>
            <li>
             5 = fibroblast
            </li>
            <li>
             6 = muscle
            </li>
            <li>
             7 = endothelial
            </li>
           </ul>
           <p>
            As part of pre-processing, the following steps are executed.
           </p>
           <ul>
            <li>
             Crop and Extract each nuclei Image + Label (128x128) based on the centroid given in the dataset.
            </li>
            <li>
             Combine classes 3 &amp; 4 into the epithelial class and 5,6 &amp; 7 into the spindle-shaped class.
            </li>
            <li>
             Update the label index for the target nuclie based on the class value
            </li>
            <li>
             Other cells which are part of the patch are modified to have label idex = 255
            </li>
           </ul>
           <p>
            Example dataset.json
           </p>
           <pre><code class="language-json">{
  "training": [
    {
      "image": "/workspace/data/CoNSePNuclei/Train/Images/train_1_3_0001.png",
      "label": "/workspace/data/CoNSePNuclei/Train/Labels/train_1_3_0001.png",
      "nuclei_id": 1,
      "mask_value": 3,
      "centroid": [
        64,
        64
      ]
    }
  ],
  "validation": [
    {
      "image": "/workspace/data/CoNSePNuclei/Test/Images/test_1_3_0001.png",
      "label": "/workspace/data/CoNSePNuclei/Test/Labels/test_1_3_0001.png",
      "nuclei_id": 1,
      "mask_value": 3,
      "centroid": [
        64,
        64
      ]
    }
  ]
}
</code></pre>
           <h2>
            Input and output formats
           </h2>
           <h3>
            Input: 5 channels
           </h3>
           <ul>
            <li>
             3 RGB channels
            </li>
            <li>
             +ve signal channel (this nuclei)
            </li>
            <li>
             -ve signal channel (other nuclei)
            </li>
           </ul>
           <h3>
            Output: 2 channels
           </h3>
           <ul>
            <li>
             0 = Background
            </li>
            <li>
             1 = Nuclei
            </li>
           </ul>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclick_annotation/docs/images/train_in_out.jpeg"/>
           </p>
           <h2>
            Scores
           </h2>
           <p>
            This model achieves the following Dice score on the validation data provided as part of the dataset:
           </p>
           <ul>
            <li>
             Train Dice score = 0.89
            </li>
            <li>
             Validation Dice score = 0.85
            </li>
           </ul>
           <h2>
            Training Performance
           </h2>
           <p>
            A graph showing the training Loss and Dice over 50 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclick_annotation/docs/images/train_loss.jpeg"/>
            <br/>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclick_annotation/docs/images/train_dice.jpeg"/>
            <br/>
           </p>
           <h2>
            Validation Performance
           </h2>
           <p>
            A graph showing the validation mean Dice over 50 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/pathology_nuclick_annotation/docs/images/val_dice.jpeg"/>
            <br/>
           </p>
           <h2>
            commands example
           </h2>
           <p>
            Execute training:
           </p>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </p>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training related options depend on the actual running environment, thus you may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            or do some other necessary changes according to the machine you used.
Please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            for more details.
           </p>
           <p>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Override the
            <code>
             train
            </code>
            config and
            <code>
             evaluate
            </code>
            config to execute multi-GPU evaluation:
           </p>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Execute inference:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            Disclaimer
           </h1>
           <p>
            This is an example, not to be used for diagnostic purposes.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Koohbanani, Navid Alemi, et al. "NuClick: a deep learning framework for interactive segmentation of microscopic images." Medical Image Analysis 65 (2020): 101771. https://arxiv.org/abs/2005.14511.
           </p>
           <p>
            [2] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y-W. Tsang, J. T. Kwak and N. Rajpoot. "HoVer-Net: Simultaneous Segmentation and Classification of Nuclei in Multi-Tissue Histology Images." Medical Image Analysis, Sept. 2019. [
            <a href="https://doi.org/10.1016/j.media.2019.101563">
             doi
            </a>
            ]
           </p>
           <p>
            [3] NuClick
            <a href="https://github.com/mostafajahanifar/nuclick_torch">
             PyTorch
            </a>
            Implementation
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_nuclick_annotation_v0.0.5.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Pathology tumor detection
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for metastasis detection on Camelyon 16 dataset.
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Pathology tumor detection
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_tumor_detection_v0.4.7.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for metastasis detection on Camelyon 16 dataset.
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           274
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           43.7MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.4.7
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for automated detection of metastases in whole-slide histopathology images.
           </p>
           <p>
            The model is trained based on ResNet18 [1] with the last fully connected layer replaced by a 1x1 convolution layer.
            <img alt="Diagram showing the flow from model input, through the model architecture, and to model output" src="http://developer.download.nvidia.com/assets/Clara/Images/clara_pt_pathology_metastasis_detection_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            All the data used to train, validate, and test this model is from
            <a href="https://camelyon16.grand-challenge.org/">
             Camelyon-16 Challenge
            </a>
            . You can download all the images for "CAMELYON16" data set from various sources listed
            <a href="https://camelyon17.grand-challenge.org/Data/">
             here
            </a>
            .
           </p>
           <p>
            Location information for training/validation patches (the location on the whole slide image where patches are extracted) are adopted from
            <a href="https://github.com/baidu-research/NCRF/tree/master/coords">
             NCRF/coords
            </a>
            .
           </p>
           <p>
            Annotation information are adopted from
            <a href="https://github.com/baidu-research/NCRF/tree/master/jsons">
             NCRF/jsons
            </a>
            .
           </p>
           <ul>
            <li>
             Target: Tumor
            </li>
            <li>
             Task: Detection
            </li>
            <li>
             Modality: Histopathology
            </li>
            <li>
             Size: 270 WSIs for training/validation, 48 WSIs for testing
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            This bundle expects the training/validation data (whole slide images) reside in a
            <code>
             {dataset_dir}/training/images
            </code>
            . By default
            <code>
             dataset_dir
            </code>
            is pointing to
            <code>
             /workspace/data/medical/pathology/
            </code>
            You can modify
            <code>
             dataset_dir
            </code>
            in the bundle config files to point to a different directory.
           </p>
           <p>
            To reduce the computation burden during the inference, patches are extracted only where there is tissue and ignoring the background according to a tissue mask. Please also create a directory for prediction output. By default
            <code>
             output_dir
            </code>
            is set to
            <code>
             eval
            </code>
            folder under the bundle root.
           </p>
           <p>
            Please refer to "Annotation" section of
            <a href="https://camelyon17.grand-challenge.org/Data/">
             Camelyon challenge
            </a>
            to prepare ground truth images, which are needed for FROC computation. By default, this data set is expected to be at
            <code>
             /workspace/data/medical/pathology/ground_truths
            </code>
            . But it can be modified in
            <code>
             evaluate_froc.sh
            </code>
            .
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             Config file: train.config
            </li>
            <li>
             GPU: at least 16 GB of GPU memory.
            </li>
            <li>
             Actual Model Input: 224 x 224 x 3
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Novograd
            </li>
            <li>
             Learning Rate: 1e-3
            </li>
            <li>
             Loss: BCEWithLogitsLoss
            </li>
            <li>
             Whole slide image reader: cuCIM (if running on Windows or Mac, please install
             <code>
              OpenSlide
             </code>
             on your system and change
             <code>
              wsi_reader
             </code>
             to "OpenSlide")
            </li>
           </ul>
           <h3>
            Input
           </h3>
           <p>
            The training pipeline is a json file (dataset.json) which includes path to each WSI, the location and the label information for each training patch.
           </p>
           <h3>
            Output
           </h3>
           <p>
            A probability number of the input patch being tumor or normal.
           </p>
           <h3>
            Inference on a WSI
           </h3>
           <p>
            Inference is performed on WSI in a sliding window manner with specified stride. A foreground mask is needed to specify the region where the inference will be performed on, given that background region which contains no tissue at all can occupy a significant portion of a WSI. Output of the inference pipeline is a probability map of size 1/stride of original WSI size.
           </p>
           <h2>
            Performance
           </h2>
           <p>
            FROC score is used for evaluating the performance of the model. After inference is done,
            <code>
             evaluate_froc.sh
            </code>
            needs to be run to evaluate FROC score based on predicted probability map (output of inference) and the ground truth tumor masks.
This model achieve the 0.91 accuracy on validation patches, and FROC of 0.72 on the 48 Camelyon testing data that have ground truth annotations available.
           </p>
           <p>
            <img alt="A Graph showing Train Acc, Train Loss, and Validation Acc" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_pathology_tumor_detection_train_and_val_metrics_v3.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Execute inference
           </h4>
           <pre><code>CUDA_LAUNCH_BLOCKING=1 python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Evaluate FROC metric
           </h4>
           <pre><code>cd scripts &amp;&amp; source evaluate_froc.sh
</code></pre>
           <h4>
            Export checkpoint to TorchScript file
           </h4>
           <p>
            TorchScript conversion is currently not supported.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] He, Kaiming, et al, "Deep Residual Learning for Image Recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.
            <a href="https://arxiv.org/pdf/1512.03385.pdf">
             https://arxiv.org/pdf/1512.03385.pdf
            </a>
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/pathology_tumor_detection_v0.4.7.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Prostate mri anatomy
      </h3>
      <h5 class="text-brand-primary">
       Keno Bressem
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for volumetric (3D) segmentation of the prostate from MRI images
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Prostate mri anatomy
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/prostate_mri_anatomy_v0.3.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for volumetric (3D) segmentation of the prostate from MRI images
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Keno Bressem
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Adams, L. C., Makowski, M. R., Engel, G., Rattunde, M., Busch, F., Asbach, P., ... &amp; Bressem, K. K. (2022). Prostate158-An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection. Computers in Biology and Medicine, 148, 105817.
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           360
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           268.9MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Prostate MRI zonal segmentation
           </h1>
           <h3>
            <strong>
             Authors
            </strong>
           </h3>
           <p>
            Lisa C. Adams, Keno K. Bressem
           </p>
           <h3>
            <strong>
             Tags
            </strong>
           </h3>
           <p>
            Segmentation, MR, Prostate
           </p>
           <h2>
            <strong>
             Model Description
            </strong>
           </h2>
           <p>
            This model was trained with the UNet architecture [1] and is used for 3D volumetric segmentation of the anatomical prostate zones on T2w MRI images. The segmentation of the anatomical regions is formulated as a voxel-wise classification. Each voxel is classified as either central gland (1), peripheral zone (2), or background (0). The model is optimized using a gradient descent method that minimizes the focal soft-dice loss between the predicted mask and the actual segmentation.
           </p>
           <h2>
            <strong>
             Data
            </strong>
           </h2>
           <p>
            The model was trained in the prostate158 training data, which is available at https://doi.org/10.5281/zenodo.6481141. Only T2w images were used for this task.
           </p>
           <h3>
            <strong>
             Preprocessing
            </strong>
           </h3>
           <p>
            MRI images in the prostate158 dataset were preprocessed, including center cropping and resampling. When applying the model to new data, this preprocessing should be repeated.
           </p>
           <h4>
            <strong>
             Center cropping
            </strong>
           </h4>
           <p>
            T2w images were acquired with a voxel spacing of 0.47 x 0.47 x 3 mm and an axial FOV size of 180 x 180 mm. However, the prostate rarely exceeds an axial diameter of 100 mm, and for zonal segmentation, the tissue surrounding the prostate is not of interest and only increases the image size and thus the computational cost. Center-cropping can reduce the image size without sacrificing information.
           </p>
           <p>
            The script
            <code>
             center_crop.py
            </code>
            allows to reproduce center-cropping as performed in the prostate158 paper.
           </p>
           <pre><code class="language-bash">python scripts/center_crop.py --file_name path/to/t2_image --out_name cropped_t2
</code></pre>
           <h4>
            <strong>
             Resampling
            </strong>
           </h4>
           <p>
            DWI and ADC sequences in prostate158 were resampled to the orientation and voxel spacing of the T2w sequence. As the zonal segmentation uses T2w images, no additional resampling is nessecary. However, the training script will perform additonal resampling automatically.
           </p>
           <h2>
            <strong>
             Performance
            </strong>
           </h2>
           <p>
            The model achives the following performance on the prostate158 test dataset:
           </p>
           <table border="1" frame="void" rules="rows">
            <thead>
             <tr>
              <td>
              </td>
              <td colspan="3">
               <b>
                <center>
                 Rater 1
                </center>
               </b>
              </td>
              <td>
              </td>
              <td colspan="3">
               <b>
                <center>
                 Rater 2
                </center>
               </b>
              </td>
             </tr>
             <tr>
              <th>
               Metric
              </th>
              <th>
               Transitional Zone
              </th>
              <th>
               Peripheral Zone
              </th>
              <th>
              </th>
              <th>
               Transitional Zone
              </th>
              <th>
               Peripheral Zone
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td>
               <a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient">
                Dice Coefficient
               </a>
              </td>
              <td>
               0.877
              </td>
              <td>
               0.754
              </td>
              <td>
              </td>
              <td>
               0.875
              </td>
              <td>
               0.730
              </td>
             </tr>
             <tr>
              <td>
               <a href="https://en.wikipedia.org/wiki/Hausdorff_distance">
                Hausdorff Distance
               </a>
              </td>
              <td>
               18.3
              </td>
              <td>
               22.8
              </td>
              <td>
              </td>
              <td>
               17.5
              </td>
              <td>
               33.2
              </td>
             </tr>
             <tr>
              <td>
               <a href="https://github.com/deepmind/surface-distance">
                Surface Distance
               </a>
              </td>
              <td>
               2.19
              </td>
              <td>
               1.95
              </td>
              <td>
              </td>
              <td>
               2.59
              </td>
              <td>
               1.88
              </td>
             </tr>
            </tbody>
           </table>
           <p>
            For more details, please see the original
            <a href="https://doi.org/10.1016/j.compbiomed.2022.105817">
             publication
            </a>
            or official
            <a href="https://github.com/kbressem/prostate158">
             GitHub repository
            </a>
           </p>
           <h2>
            <strong>
             System Configuration
            </strong>
           </h2>
           <p>
            The model was trained for 100 epochs on a workstaion with a single Nvidia RTX 3080 GPU. This takes approximatly 8 hours.
           </p>
           <h2>
            <strong>
             Limitations
            </strong>
            (Optional)
           </h2>
           <p>
            This training and inference pipeline was developed for research purposes only. This research use only software that has not been cleared or approved by FDA or any regulatory agency. The model is for research/developmental purposes only and cannot be used directly for clinical procedures.
           </p>
           <h2>
            <strong>
             Citation Info
            </strong>
            (Optional)
           </h2>
           <pre><code>@article{ADAMS2022105817,
title = {Prostate158 - An expert-annotated 3T MRI dataset and algorithm for prostate cancer detection},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105817},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105817},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005789},
author = {Lisa C. Adams and Marcus R. Makowski and Günther Engel and Maximilian Rattunde and Felix Busch and Patrick Asbach and Stefan M. Niehues and Shankeeth Vinayahalingam and Bram {van Ginneken} and Geert Litjens and Keno K. Bressem},
keywords = {Prostate cancer, Deep learning, Machine learning, Artificial intelligence, Magnetic resonance imaging, Biparametric prostate MRI}
}
</code></pre>
           <h2>
            <strong>
             References
            </strong>
           </h2>
           <p>
            [1] Sakinis, Tomas, et al. "Interactive segmentation of medical images through fully convolutional neural networks." arXiv preprint arXiv:1903.08205 (2019).
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/prostate_mri_anatomy_v0.3.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Renalstructures unest segmentation
      </h3>
      <h5 class="text-brand-primary">
       Vanderbilt University + MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A transformer-based model for renal segmentation from CT image
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Renalstructures unest segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/renalStructures_UNEST_segmentation_v0.2.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A transformer-based model for renal segmentation from CT image
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Vanderbilt University + MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Tang, Yucheng, et al. 'Self-supervised pre-training of swin transformers for 3d medical image analysis. arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791.
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           437
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           309.0MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.2.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Description
           </h1>
           <p>
            A pre-trained model for training and inferencing volumetric (3D) kidney substructures segmentation from contrast-enhanced CT images (Arterial/Portal Venous Phase). Training pipeline is provided to support model fine-tuning with bundle and MONAI Label active learning.
           </p>
           <p>
            A tutorial and release of model for kidney cortex, medulla and collecting system segmentation.
           </p>
           <p>
            Authors: Yinchi Zhou (yinchi.zhou@vanderbilt.edu) | Xin Yu (xin.yu@vanderbilt.edu) | Yucheng Tang (yuchengt@nvidia.com) |
           </p>
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained UNEST base model [1] for volumetric (3D) renal structures segmentation using dynamic contrast enhanced arterial or venous phase CT images.
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training data is from the [ImageVU RenalSeg dataset] from Vanderbilt University and Vanderbilt University Medical Center.
(The training data is not public available yet).
           </p>
           <ul>
            <li>
             Target: Renal Cortex | Medulla | Pelvis Collecting System
            </li>
            <li>
             Task: Segmentation
            </li>
            <li>
             Modality: CT (Artrial | Venous phase)
            </li>
            <li>
             Size: 96 3D volumes
            </li>
           </ul>
           <p>
            The data and segmentation demonstration is as follow:
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/renalStructures_UNEST_segmentation/docs/./renal.png"/>
            <br/>
           </p>
           <h2>
            Method and Network
           </h2>
           <p>
            The UNEST model is a 3D hierarchical transformer-based semgnetation network.
           </p>
           <p>
            Details of the architecture:
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/renalStructures_UNEST_segmentation/docs/./unest.png"/>
            <br/>
           </p>
           <h2>
            Training configuration
           </h2>
           <p>
            The training was performed with at least one 16GB-memory GPU.
           </p>
           <p>
            Actual Model Input: 96 x 96 x 96
           </p>
           <h2>
            Input and output formats
           </h2>
           <p>
            Input: 1 channel CT image
           </p>
           <p>
            Output: 4: 0:Background, 1:Renal Cortex, 2:Medulla, 3:Pelvicalyceal System
           </p>
           <h2>
            Performance
           </h2>
           <p>
            A graph showing the validation mean Dice for 5000 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/renalStructures_UNEST_segmentation/docs/./val_dice.png"/>
            <br/>
           </p>
           <p>
            This model achieves the following Dice score on the validation data (our own split from the training dataset):
           </p>
           <p>
            Mean Valdiation Dice = 0.8523
           </p>
           <p>
            Note that mean dice is computed in the original spacing of the input data.
           </p>
           <h2>
            commands example
           </h2>
           <p>
            Download trained checkpoint model to ./model/model.pt:
           </p>
           <p>
            Add scripts component:  To run the workflow with customized components, PYTHONPATH should be revised to include the path to the customized component:
           </p>
           <pre><code>export PYTHONPATH=$PYTHONPATH:"'&lt;path to the bundle root dir&gt;/scripts'"

</code></pre>
           <p>
            Execute Training:
           </p>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <p>
            Execute inference:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h2>
            More examples output
           </h2>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/renalStructures_UNEST_segmentation/docs/./demos.png"/>
            <br/>
           </p>
           <h1>
            Disclaimer
           </h1>
           <p>
            This is an example, not to be used for diagnostic purposes.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Yu, Xin, Yinchi Zhou, Yucheng Tang et al. "Characterizing Renal Structures with 3D Block Aggregate Transformers." arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf
           </p>
           <p>
            [2] Zizhao Zhang et al. "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding." AAAI Conference on Artificial Intelligence (AAAI) 2022
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/renalStructures_UNEST_segmentation_v0.2.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Spleen ct segmentation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for volumetric (3D) segmentation of the spleen from CT image
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Spleen ct segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/spleen_ct_segmentation_v0.3.8.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for volumetric (3D) segmentation of the spleen from CT image
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Xia, Yingda, et al. '3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training. arXiv preprint arXiv:1811.12506 (2018). https://arxiv.org/abs/1811.12506.
            </li>
            <li>
             Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           2393
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           33.9MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.8
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for volumetric (3D) segmentation of the spleen from CT images.
           </p>
           <p>
            This model is trained using the runner-up [1] awarded pipeline of the "Medical Segmentation Decathlon Challenge 2018" using the UNet architecture [2] with 32 training images and 9 validation images.
           </p>
           <p>
            <img alt="model workflow" src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_spleen_ct_segmentation_workflow.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training dataset is the Spleen Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.
           </p>
           <ul>
            <li>
             Target: Spleen
            </li>
            <li>
             Modality: CT
            </li>
            <li>
             Size: 61 3D volumes (41 Training + 20 Testing)
            </li>
            <li>
             Source: Memorial Sloan Kettering Cancer Center
            </li>
            <li>
             Challenge: Large-ranging foreground size
            </li>
           </ul>
           <h2>
            Training configuration
           </h2>
           <p>
            The segmentation of spleen region is formulated as the voxel-wise binary classification. Each voxel is predicted as either foreground (spleen) or background. And the model is optimized with gradient descent method minimizing Dice + cross entropy loss between the predicted mask and ground truth segmentation.
           </p>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: at least 12GB of GPU memory
            </li>
            <li>
             Actual Model Input: 96 x 96 x 96
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: Adam
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: DiceCELoss
            </li>
           </ul>
           <h3>
            Input
           </h3>
           <p>
            One channel
- CT image
           </p>
           <h3>
            Output
           </h3>
           <p>
            Two channels
- Label 1: spleen
- Label 0: everything else
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Dice score is used for evaluating the performance of the model. This model achieves a mean dice score of 0.96.
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="A graph showing the training loss over 1260 epochs (10080 iterations)." src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_spleen_ct_segmentation_train_2.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            <img alt="A graph showing the validation mean Dice over 1260 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/clara_pt_spleen_ct_segmentation_val_2.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config and
            <code>
             evaluate
            </code>
            config to execute multi-GPU evaluation:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] Xia, Yingda, et al. "3D Semi-Supervised Learning with Uncertainty-Aware Multi-View Co-Training." arXiv preprint arXiv:1811.12506 (2018). https://arxiv.org/abs/1811.12506.
           </p>
           <p>
            [2] Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/spleen_ct_segmentation_v0.3.8.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Spleen deepedit annotation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       This is a pre-trained model for 3D segmentation of the spleen organ from CT images using DeepEdit.
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Spleen deepedit annotation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/spleen_deepedit_annotation_v0.3.7.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           This is a pre-trained model for 3D segmentation of the spleen organ from CT images using DeepEdit.
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Sakinis, Tomas, et al. 'Interactive segmentation of medical images through fully convolutional neural networks.' arXiv preprint arXiv:1903.08205 (2019)
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           1294
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           219.1MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.7
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained model for 3D segmentation of the spleen organ from CT images using DeepEdit.
           </p>
           <p>
            DeepEdit is an algorithm that combines the power of two models in one single architecture. It allows the user to perform inference as a standard segmentation method (i.e., UNet) and interactively segment part of an image using clicks [2]. DeepEdit aims to facilitate the user experience and, at the same time, develop new active learning techniques.
           </p>
           <p>
            The model was trained on 32 images and validated on 9 images.
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training dataset is the Spleen Task from the Medical Segmentation Decathalon. Users can find more details on the datasets at http://medicaldecathlon.com/.
           </p>
           <ul>
            <li>
             Target: Spleen
            </li>
            <li>
             Modality: CT
            </li>
            <li>
             Size: 61 3D volumes (41 Training + 20 Testing)
            </li>
            <li>
             Source: Memorial Sloan Kettering Cancer Center
            </li>
            <li>
             Challenge: Large-ranging foreground size
            </li>
           </ul>
           <h2>
            Training configuration
           </h2>
           <p>
            The training as performed with the following:
- GPU: at least 12GB of GPU memory
- Actual Model Input: 128 x 128 x 128
- AMP: True
- Optimizer: Adam
- Learning Rate: 1e-4
- Loss: DiceCELoss
           </p>
           <h3>
            Input
           </h3>
           <p>
            Three channels
- CT image
- Spleen Segment
- Background Segment
           </p>
           <h3>
            Output
           </h3>
           <p>
            Two channels
- Label 1: spleen
- Label 0: everything else
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Dice score is used for evaluating the performance of the model. This model achieves a dice score of greater than 0.90, depending on the number of simulated clicks.
           </p>
           <h4>
            Training Dice
           </h4>
           <p>
            <img alt="A graph showing the train dice over 90 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_train_dice.png"/>
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="A graph showing the training loss over 90 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_train_loss.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            <img alt="A graph showing the validation dice over 90 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_spleen_deepedit_annotation_val_dice.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] Diaz-Pinto, Andres, et al. DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images. MICCAI Workshop on Data Augmentation, Labelling, and Imperfections. MICCAI 2022.
           </p>
           <p>
            [2] Diaz-Pinto, Andres, et al. "MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images." arXiv preprint arXiv:2203.12362 (2022).
           </p>
           <p>
            [3] Sakinis, Tomas, et al. "Interactive segmentation of medical images through fully convolutional neural networks." arXiv preprint arXiv:1903.08205 (2019).
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/spleen_deepedit_annotation_v0.3.7.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Swin unetr btcv segmentation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained model for volumetric (3D) multi-organ segmentation from CT image
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Swin unetr btcv segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/swin_unetr_btcv_segmentation_v0.4.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained model for volumetric (3D) multi-organ segmentation from CT image
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Hatamizadeh, Ali, et al. 'Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266 (2022). https://arxiv.org/abs/2201.01266.
            </li>
            <li>
             Tang, Yucheng, et al. 'Self-supervised pre-training of swin transformers for 3d medical image analysis. arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791.
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           1681
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           220.2MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.4.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained Swin UNETR [1,2] for volumetric (3D) multi-organ segmentation using CT images from Beyond the Cranial Vault (BTCV) Segmentation Challenge dataset [3].
           </p>
           <p>
            <img alt="model workflow" src="https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_workflow_v1.png"/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training data is from the
            <a href="https://www.synapse.org/#!Synapse:syn3193805/wiki/89480/">
             BTCV dataset
            </a>
            (Register through
            <code>
             Synapse
            </code>
            and download the
            <code>
             Abdomen/RawData.zip
            </code>
            ).
           </p>
           <ul>
            <li>
             Target: Multi-organs
            </li>
            <li>
             Task: Segmentation
            </li>
            <li>
             Modality: CT
            </li>
            <li>
             Size: 30 3D volumes (24 Training + 6 Testing)
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            The dataset format needs to be redefined using the following commands:
           </p>
           <pre><code>unzip RawData.zip
mv RawData/Training/img/ RawData/imagesTr
mv RawData/Training/label/ RawData/labelsTr
mv RawData/Testing/img/ RawData/imagesTs
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            The training as performed with the following:
- GPU: At least 32GB of GPU memory
- Actual Model Input: 96 x 96 x 96
- AMP: True
- Optimizer: Adam
- Learning Rate: 2e-4
           </p>
           <h3>
            Input
           </h3>
           <p>
            1 channel
- CT image
           </p>
           <h3>
            Output
           </h3>
           <p>
            14 channels:
- 0: Background
- 1: Spleen
- 2: Right Kidney
- 3: Left Kideny
- 4: Gallbladder
- 5: Esophagus
- 6: Liver
- 7: Stomach
- 8: Aorta
- 9: IVC
- 10: Portal and Splenic Veins
- 11: Pancreas
- 12: Right adrenal gland
- 13: Left adrenal gland
           </p>
           <h2>
            Performance
           </h2>
           <p>
            Dice score was used for evaluating the performance of the model. This model achieves a mean dice score of 0.8269
           </p>
           <h4>
            Training Loss
           </h4>
           <p>
            <img alt="The figure shows the training loss curve for 10K iterations." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_trainloss_v1.png"/>
           </p>
           <h4>
            Validation Dice
           </h4>
           <p>
            <img alt="A graph showing the validation mean Dice for 5000 epochs." src="https://developer.download.nvidia.com/assets/Clara/Images/monai_swin_unetr_btcv_segmentation_validation_meandice_v1.png"/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training:
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training:
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference:
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Export checkpoint to TorchScript file:
           </h4>
           <p>
            TorchScript conversion is currently not supported.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Hatamizadeh, Ali, et al. "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images." arXiv preprint arXiv:2201.01266 (2022). https://arxiv.org/abs/2201.01266.
           </p>
           <p>
            [2] Tang, Yucheng, et al. "Self-supervised pre-training of swin transformers for 3d medical image analysis." arXiv preprint arXiv:2111.14791 (2021). https://arxiv.org/abs/2111.14791.
           </p>
           <p>
            [3] Landman B, et al. "MICCAI multi-atlas labeling beyond the cranial vault–workshop and challenge." In Proc. of the MICCAI Multi-Atlas Labeling Beyond Cranial Vault—Workshop Challenge 2015 Oct (Vol. 5, p. 12).
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/swin_unetr_btcv_segmentation_v0.4.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Valve landmarks
      </h3>
      <h5 class="text-brand-primary">
       Eric Kerfoot
      </h5>
      <p class="pt-2 pb-10 text-sm">
       This network is used to find where valves attach to heart to help construct 3D FEM models for computation. The output is an array of 10 2D coordinates.
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Valve landmarks
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/valve_landmarks_v0.4.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           This network is used to find where valves attach to heart to help construct 3D FEM models for computation. The output is an array of 10 2D coordinates.
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Eric Kerfoot
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Kerfoot, E, King, CE, Ismail, T, Nordsletten, D &amp; Miller, R 2021, Estimation of Cardiac Valve Annuli Motion with Deep Learning. https://doi.org/10.1007/978-3-030-68107-4_15
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           217
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           14.1MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.4.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            2D Cardiac Valve Landmark Regressor
           </h1>
           <p>
            This network identifies 10 different landmarks in 2D+t MR images of the heart (2 chamber, 3 chamber, and 4 chamber) representing the insertion locations of valve leaflets into the myocardial wall. These coordinates are used in part of the construction of 3D FEM cardiac models suitable for physics simulation of heart functions.
           </p>
           <p>
            Input images are individual 2D slices from the time series, and the output from the network is a
            <code>
             (2, 10)
            </code>
            set of 2D points in
            <code>
             HW
            </code>
            image coordinate space. The 10 coordinates correspond to the attachment point for these valves:
           </p>
           <ol>
            <li>
             Mitral anterior in 2CH
            </li>
            <li>
             Mitral posterior in 2CH
            </li>
            <li>
             Mitral septal in 3CH
            </li>
            <li>
             Mitral free wall in 3CH
            </li>
            <li>
             Mitral septal in 4CH
            </li>
            <li>
             Mitral free wall in 4CH
            </li>
            <li>
             Aortic septal
            </li>
            <li>
             Aortic free wall
            </li>
            <li>
             Tricuspid septal
            </li>
            <li>
             Tricuspid free wall
            </li>
           </ol>
           <p>
            Landmarks which do not appear in a particular image are predicted to be
            <code>
             (0, 0)
            </code>
            or close to this location. The mitral valve is expected to appear in all three views. Landmarks are not provided for the pulmonary valve.
           </p>
           <p>
            Example plot of landmarks on a single frame, see
            <a href="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/valve_landmarks/docs/./view_results.ipynb">
             view_results.ipynb
            </a>
            for visualising network output:
           </p>
           <p>
            <img alt="Landmark Example Image" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/valve_landmarks/docs/./prediction_example.png"/>
           </p>
           <h2>
            Training
           </h2>
           <p>
            The training script
            <code>
             train.json
            </code>
            is provided to train the network using a dataset of image pairs containing the MR image and a landmark image. This is done to reuse image-based transforms which do not currently operate on geometry. A number of other transforms are provided in
            <code>
             valve_landmarks.py
            </code>
            to implement Fourier-space dropout, image shifting which preserve landmarks, and smooth-field deformation applied to images and landmarks.
           </p>
           <p>
            The dataset used for training unfortunately cannot be made public, however the training script can be used with any NPZ file containing the training image stack in key
            <code>
             trainImgs
            </code>
            and landmark image stack in
            <code>
             trainLMImgs
            </code>
            , plus
            <code>
             testImgs
            </code>
            and
            <code>
             testLMImgs
            </code>
            containing validation data. The landmark images are defined as 0 for every non-landmark pixel, with landmark pixels contaning the following values for each landmark type:
           </p>
           <ul>
            <li>
             10: Mitral anterior in 2CH
            </li>
            <li>
             15: Mitral posterior in 2CH
            </li>
            <li>
             20: Mitral septal in 3CH
            </li>
            <li>
             25: Mitral free wall in 3CH
            </li>
            <li>
             30: Mitral septal in 4CH
            </li>
            <li>
             35: Mitral free wall in 4CH
            </li>
            <li>
             100: Aortic septal
            </li>
            <li>
             150: Aortic free wall
            </li>
            <li>
             200: Tricuspid septal
            </li>
            <li>
             250: Tricuspid free wall
            </li>
           </ul>
           <p>
            The following command will train with the default NPZ filename
            <code>
             ./valvelandmarks.npz
            </code>
            , assuming the current directory is the bundle directory:
           </p>
           <pre><code class="language-sh">python -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json', 'configs/common.json']" \
    --bundle_root . --dataset_file ./valvelandmarks.npz --output_dir /path/to/outputs
</code></pre>
           <h2>
            Inference
           </h2>
           <p>
            The included
            <code>
             inference.json
            </code>
            script will run inference on a directory containing Nifti files whose images have shape
            <code>
             (256, 256, 1, N)
            </code>
            for
            <code>
             N
            </code>
            timesteps. For each image the output in the
            <code>
             output_dir
            </code>
            directory will be a npy file containing a result array of shape
            <code>
             (N, 2, 10)
            </code>
            storing the 10 coordinates for each
            <code>
             N
            </code>
            timesteps. Invoking this script can be done as follows, assuming the current directory is the bundle directory:
           </p>
           <pre><code class="language-sh">python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/inference.json', 'configs/common.json']" \
    --bundle_root . --dataset_dir /path/to/data --output_dir /path/to/outputs
</code></pre>
           <p>
            The provided test Nifti file can be placed in a directory which is then used as the
            <code>
             dataset_dir
            </code>
            value. This image was derived from
            <a href="http://www.cardiacatlas.org/studies/amrg-cardiac-atlas">
             the AMRG Cardiac Atlas dataset
            </a>
            (AMRG Cardiac Atlas, Auckland MRI Research Group, Auckland, New Zealand). The results from this inference can be visualised by changing path values in
            <a href="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/valve_landmarks/docs/./view_results.ipynb">
             view_results.ipynb
            </a>
            .
           </p>
           <h3>
            Reference
           </h3>
           <p>
            The work for this model and its application is described in:
           </p>
           <p>
            <code>
             Kerfoot, E, King, CE, Ismail, T, Nordsletten, D &amp; Miller, R 2021, Estimation of Cardiac Valve Annuli Motion with Deep Learning. in E Puyol Anton, M Pop, M Sermesant, V Campello, A Lalande, K Lekadir, A Suinesiaputra, O Camara &amp; A Young (eds), Statistical Atlases and Computational Models of the Heart. MandMs and EMIDEC Challenges - 11th International Workshop, STACOM 2020, Held in Conjunction with MICCAI 2020, Revised Selected Papers. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 12592 LNCS, Springer Science and Business Media Deutschland GmbH, pp. 146-155, 11th International Workshop on Statistical Atlases and Computational Models of the Heart, STACOM 2020 held in Conjunction with MICCAI 2020, Lima, Peru, 4/10/2020. https://doi.org/10.1007/978-3-030-68107-4_15
            </code>
           </p>
           <h1>
            License
           </h1>
           <p>
            This model is released under the MIT License. The license file is included with the model.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/valve_landmarks_v0.4.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Ventricular short axis 3label
      </h3>
      <h5 class="text-brand-primary">
       Eric Kerfoot
      </h5>
      <p class="pt-2 pb-10 text-sm">
       This network segments full cycle short axis images of the ventricles, labelling LV pool separate from myocardium and RV pool
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Ventricular short axis 3label
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/ventricular_short_axis_3label_v0.3.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           This network segments full cycle short axis images of the ventricles, labelling LV pool separate from myocardium and RV pool
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Eric Kerfoot
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           254
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           11.8MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.3.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            3 Label Ventricular Segmentation
           </h1>
           <p>
            This network segments cardiac ventricle in 2D short axis MR images. The left ventricular pool is class 1, left ventricular myocardium class 2, and right ventricular pool class 3. Full cycle segmentation with this network is possible although much of the training data is composed of segmented end-diastole images. The input to the network is single 2D images thus segmenting whole time-dependent volumes consists of multiple inference operations.
           </p>
           <p>
            The network and training scheme are essentially identical to that described in:
           </p>
           <p>
            <code>
             Kerfoot E., Clough J., Oksuz I., Lee J., King A.P., Schnabel J.A. (2019) Left-Ventricle Quantification Using Residual U-Net. In: Pop M. et al. (eds) Statistical Atlases and Computational Models of the Heart. Atrial Segmentation and LV Quantification Challenges. STACOM 2018. Lecture Notes in Computer Science, vol 11395. Springer, Cham. https://doi.org/10.1007/978-3-030-12029-0_40
            </code>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The dataset used to train this network unfortunately cannot be made public as it contains unreleased image data from King's College London. Existing public datasets such as the
            <a href="http://www.cardiacatlas.org/studies/sunnybrook-cardiac-data/">
             Sunnybrook Cardiac Dataset
            </a>
            and
            <a href="https://www.creatis.insa-lyon.fr/Challenge/acdc/">
             ACDC Challenge
            </a>
            set can be used to train a similar network.
           </p>
           <p>
            The
            <code>
             train.json
            </code>
            configuration assumes all data is stored in a single npz file with keys "images" and "segs" containing respectively the raw image data and their accompanying segmentations. The given network was training with stored volumes with shapes
            <code>
             (9095, 256, 256)
            </code>
            thus other data of differing spatial dimensions must be cropped to
            <code>
             (256, 256)
            </code>
            or zero-padded to that size. For the training data this was done as a preprocessing step but the original pixel values are otherwise unchanged from their original forms.
           </p>
           <h2>
            Training
           </h2>
           <p>
            The network is trained with this data in conjunction with a series of augmentations for regularisation and robustness. Many of the original images are smaller than the expected size of
            <code>
             (256, 256)
            </code>
            and so were zero-padded, the network can thus be expected to be robust against large amounts of empty space in the inputs. Rotation and zooming is also applied to force the network to learn different sizes and orientations of the heart in the field of view.
           </p>
           <p>
            Free-form deformation is applied to vary the shape of the heart and its surrounding tissues which mimics to a degree deformation like what would be observed through the cardiac cycle. This of course does not replicate the heart moving through plane during the cycle or represent other observed changes but does provide enough variation that full-cycle segmentation is generally acceptable.
           </p>
           <p>
            Smooth fields are used to vary contrast and intensity in localised regions to simulate some of the variation in image quality caused by acquisition artefacts. Guassian noise is also added to simulate poor quality acquisition. These together force the network to learn to deal with a wider variation of image quality and partially to account for the difference between scanner vendors.
           </p>
           <p>
            Training is invoked with the following command line:
           </p>
           <pre><code class="language-sh">python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf --bundle_root .
</code></pre>
           <p>
            The dataset file is assumed to be
            <code>
             allimages3label.npz
            </code>
            but can be changed by setting the
            <code>
             dataset_file
            </code>
            value to your own file.
           </p>
           <h2>
            Inference
           </h2>
           <p>
            An example notebook
            <a href="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/ventricular_short_axis_3label/docs/./visualise.ipynb">
             visualise.ipynb
            </a>
            demonstrates using the network directly with input images. Inference of 3D volumes only can be accomplished with the
            <code>
             inference.json
            </code>
            configuration:
           </p>
           <pre><code class="language-sh">python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf --dataset_dir dataset --output_dir ./output/ --bundle_root .
</code></pre>
           <h1>
            License
           </h1>
           <p>
            This model is released under the MIT License. The license file is included with the model.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/ventricular_short_axis_3label_v0.3.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Wholebody ct segmentation
      </h3>
      <h5 class="text-brand-primary">
       MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A pre-trained SegResNet model for volumetric (3D) segmentation of the 104 whole body segments
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Wholebody ct segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/wholeBody_ct_segmentation_v0.1.0.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A pre-trained SegResNet model for volumetric (3D) segmentation of the 104 whole body segments
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868.
            </li>
            <li>
             Myronenko, A., Siddiquee, M.M.R., Yang, D., He, Y. and Xu, D., 2022. Automated head and neck tumor segmentation from 3D PET/CT. arXiv preprint arXiv:2209.10809.
            </li>
            <li>
             Tang, Y., Gao, R., Lee, H.H., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Abramson, R.G. and Bao, S., 2021. High-resolution 3D abdominal segmentation with random patch network fusion. Medical image analysis, 69, p.101894.
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           46
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           133.3MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.1.0
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Model Overview
           </h1>
           <p>
            Body CT segmentation models are evolving. Starting from abdominal multi-organ segmentation model [1]. Now the community is developing hundreds of target anatomies. In this bundle, we provide re-trained models for (3D) segmentation of 104 whole-body segments.
           </p>
           <p>
            This model is trained using the SegResNet [3] network. The model is trained using TotalSegmentator datasets [2].
           </p>
           <p>
            <img alt="structures" src="https://github.com/wasserth/TotalSegmentator/blob/master/resources/imgs/overview_classes.png"/>
           </p>
           <p>
            Figure source from the TotalSegmentator [2].
           </p>
           <h2>
            MONAI Label Showcase
           </h2>
           <ul>
            <li>
             We highlight the use of this bundle to use and visualize in MONAI Label + 3D Slicer integration.
            </li>
           </ul>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBody_ct_segmentation/docs/./imgs/totalsegmentator_monailabel.png"/>
            <br/>
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training set is the 104 whole-body structures from the TotalSegmentator released datasets. Users can find more details on the datasets at https://github.com/wasserth/TotalSegmentator. All rights and licenses are reserved to the original authors.
           </p>
           <ul>
            <li>
             Target: 104 structures
            </li>
            <li>
             Modality: CT
            </li>
            <li>
             Source: TotalSegmentator
            </li>
            <li>
             Challenge: Large volumes of structures in CT images
            </li>
           </ul>
           <h3>
            Preprocessing
           </h3>
           <p>
            To use the bundle, users need to download the data and merge all annotated labels into one NIFTI file. Each file contains 0-104 values, each value represents one anatomy class. A sample set is provided with this
            <a href="https://drive.google.com/file/d/1DtDmERVMjks1HooUhggOKAuDm0YIEunG/view?usp=share_link">
             link
            </a>
            .
           </p>
           <h2>
            Training Configuration
           </h2>
           <p>
            The segmentation of 104 tissues is formulated as voxel-wise multi-label segmentation. The model is optimized with the gradient descent method minimizing Dice + cross-entropy loss between the predicted mask and ground truth segmentation.
           </p>
           <p>
            The training was performed with the following:
           </p>
           <ul>
            <li>
             GPU: 32 GB of GPU memory
            </li>
            <li>
             Actual Model Input: 96 x 96 x 96
            </li>
            <li>
             AMP: True
            </li>
            <li>
             Optimizer: AdamW
            </li>
            <li>
             Learning Rate: 1e-4
            </li>
            <li>
             Loss: DiceCELoss
            </li>
           </ul>
           <h3>
            Input
           </h3>
           <p>
            One channel
- CT image
           </p>
           <h3>
            Output
           </h3>
           <p>
            105 channels
- Label 0: Background (everything else)
- label 1-105: Foreground classes (104)
           </p>
           <h3>
            High-Resolution and Low-Resolution Models
           </h3>
           <p>
            We retrained two versions of the totalSegmentator models, following the original paper and implementation.
To meet multiple demands according to computation resources and performance, we provide a 1.5 mm model and a 3.0 mm model, both models are trained with 104 foreground output channels.
           </p>
           <p>
            In this bundle, we configured a parameter called
            <code>
             highres
            </code>
            , users can set it to
            <code>
             true
            </code>
            when using 1.5 mm model, and set it to
            <code>
             false
            </code>
            to use the 3.0 mm model. The high-resolution model is named
            <code>
             model.pt
            </code>
            by default, the low-resolution model is named
            <code>
             model_lowres.pt
            </code>
            .
           </p>
           <p>
            In MONAI Label use case, users can set the parameter in 3D Slicer plugin to control which model to infer and train.
           </p>
           <ul>
            <li>
             Pretrained Checkpoints
            </li>
            <li>
             1.5 mm model:
             <a href="https://drive.google.com/file/d/1PHpFWboimEXmMSe2vBra6T8SaCMC2SHT/view?usp=share_link">
              Download link
             </a>
            </li>
            <li>
             3.0 mm model:
             <a href="https://drive.google.com/file/d/1c3osYscnr6710ObqZZS8GkZJQlWlc7rt/view?usp=share_link">
              Download link
             </a>
            </li>
           </ul>
           <h3>
            Resource Requirements and Latency Benchmarks
           </h3>
           <p>
            Latencies and memory performance of using the bundle with MONAI Label:
           </p>
           <p>
            Tested Image Dimension:
            <strong>
             (512, 512, 397)
            </strong>
            , the slice thickness is
            <strong>
             1.5mm
            </strong>
            in this case. After resample to
            <strong>
             1.5
            </strong>
            isotropic resolution, the dimension is
            <strong>
             (287, 287, 397)
            </strong>
           </p>
           <h2>
            1.5 mm (highres) model (Single Model with 104 foreground classes)
           </h2>
           <p>
            Benchmarking on GPU: Memory:
            <strong>
             28.73G
            </strong>
           </p>
           <ul>
            <li>
             <code>
              ++ Latencies =&gt; Total: 6.0277; Pre: 1.6228; Inferer: 4.1153; Invert: 0.0000; Post: 0.0897; Write: 0.1995
             </code>
            </li>
           </ul>
           <p>
            Benchmarking on CPU: Memory:
            <strong>
             26G
            </strong>
           </p>
           <ul>
            <li>
             <code>
              ++ Latencies =&gt; Total: 38.3108; Pre: 1.6643; Inferer: 30.3018; Invert: 0.0000; Post: 6.1656; Write: 0.1786
             </code>
            </li>
           </ul>
           <h2>
            3.0 mm (lowres) model (single model with 104 foreground classes)
           </h2>
           <p>
            GPU: Memory:
            <strong>
             5.89G
            </strong>
           </p>
           <ul>
            <li>
             <code>
              ++ Latencies =&gt; Total: 1.9993; Pre: 1.2363; Inferer: 0.5207; Invert: 0.0000; Post: 0.0358; Write: 0.2060
             </code>
            </li>
           </ul>
           <p>
            CPU: Memory:
            <strong>
             2.3G
            </strong>
           </p>
           <ul>
            <li>
             <code>
              ++ Latencies =&gt; Total: 6.6138; Pre: 1.3192; Inferer: 3.6746; Invert: 0.0000; Post: 1.4431; Write: 0.1760
             </code>
            </li>
           </ul>
           <h2>
            Performance
           </h2>
           <ul>
            <li>
             <p>
              1.5 mm Model Training
             </p>
            </li>
            <li>
             <p>
              Training Accuracy
             </p>
            </li>
           </ul>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBody_ct_segmentation/docs/./imgs/totalsegmentator_train_accuracy.png"/>
            <br/>
           </p>
           <ul>
            <li>
             Validation Dice
            </li>
           </ul>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBody_ct_segmentation/docs/./imgs/totalsegmentator_15mm_validation.png"/>
            <br/>
           </p>
           <h2>
            MONAI Bundle Commands
           </h2>
           <p>
            In addition to the Pythonic APIs, a few command line interfaces (CLI) are provided to interact with the bundle. The CLI supports flexible use cases, such as overriding configs at runtime and predefining arguments in a file.
           </p>
           <p>
            For more details usage instructions, visit the
            <a href="https://docs.monai.io/en/latest/config_syntax.html">
             MONAI Bundle Configuration Page
            </a>
            .
           </p>
           <h4>
            Execute training
           </h4>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute multi-GPU training
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run training --meta_file configs/metadata.json --config_file "['configs/train.json','configs/multi_gpu_train.json']" --logging_file configs/logging.conf
</code></pre>
           <p>
            Please note that the distributed training-related options depend on the actual running environment; thus, users may need to remove
            <code>
             --standalone
            </code>
            , modify
            <code>
             --nnodes
            </code>
            , or do some other necessary changes according to the machine used. For more details, please refer to
            <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">
             pytorch's official tutorial
            </a>
            .
           </p>
           <h4>
            Override the
            <code>
             train
            </code>
            config to execute evaluation with the trained model
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Override the
            <code>
             train
            </code>
            config and
            <code>
             evaluate
            </code>
            config to execute multi-GPU evaluation
           </h4>
           <pre><code>torchrun --standalone --nnodes=1 --nproc_per_node=2 -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file "['configs/train.json','configs/evaluate.json','configs/multi_gpu_evaluate.json']" --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h4>
            Execute inference with Data Samples
           </h4>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf --datalist "['sampledata/imagesTr/s0037.nii.gz','sampledata/imagesTr/s0038.nii.gz']"
</code></pre>
           <h1>
            References
           </h1>
           <p>
            [1] Tang, Y., Gao, R., Lee, H.H., Han, S., Chen, Y., Gao, D., Nath, V., Bermudez, C., Savona, M.R., Abramson, R.G. and Bao, S., 2021. High-resolution 3D abdominal segmentation with random patch network fusion. Medical image analysis, 69, p.101894.
           </p>
           <p>
            [2] Wasserthal, J., Meyer, M., Breit, H.C., Cyriac, J., Yang, S. and Segeroth, M., 2022. TotalSegmentator: robust segmentation of 104 anatomical structures in CT images. arXiv preprint arXiv:2208.05868.
           </p>
           <p>
            [3] Myronenko, A., Siddiquee, M.M.R., Yang, D., He, Y. and Xu, D., 2022. Automated head and neck tumor segmentation from 3D PET/CT. arXiv preprint arXiv:2209.10809.
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/wholeBody_ct_segmentation_v0.1.0.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div class="p-6 shadow-lg rounded-lg border-2 border-neutral-lightgray relative">
      <h3 class="text-lg font-bold text-brand-primary">
       Wholebrainseg large unest segmentation
      </h3>
      <h5 class="text-brand-primary">
       Vanderbilt University + MONAI team
      </h5>
      <p class="pt-2 pb-10 text-sm">
       A 3D transformer-based model for whole brain segmentation from T1W MRI image
      </p>
      <div x-data="dialog()">
       <a class="brand-btn absolute right-3 bottom-2" x-bind="trigger">
        Model Details
       </a>
       <div class="dialog dialog-lg" x-bind="dialog" x-cloak="">
        <div class="dialog-content">
         <div class="dialog-header text-brand-primary text-3xl justify-start">
          <span class="mr-4">
           Wholebrainseg large unest segmentation
          </span>
          <a class="brand-btn m-1" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/wholeBrainSeg_Large_UNEST_segmentation_v0.2.2.zip">
           Download
          </a>
          <button @click="close" aria-label="Close" class="btn btn-light btn-sm btn-icon ml-auto" type="button">
           <svg fill="none" height="24" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
            <line x1="18" x2="6" y1="6" y2="18">
            </line>
            <line x1="6" x2="18" y1="6" y2="18">
            </line>
           </svg>
          </button>
         </div>
         <div class="dialog-body h-screen">
          <h2 class="text-xl font-bold text-brand-primary">
           Model Metadata:
          </h2>
          <p>
           <strong>
            Overview:
           </strong>
           A 3D transformer-based model for whole brain segmentation from T1W MRI image
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Author(s):
           </strong>
           Vanderbilt University + MONAI team
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            References:
           </strong>
           <ul>
            <li>
             Xin, et al. Characterizing Renal Structures with 3D Block Aggregate Transformers. arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf
            </li>
           </ul>
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Downloads:
           </strong>
           421
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            File Size:
           </strong>
           310.6MB
          </p>
          <p>
           <strong class="text-neutral-darkestblack">
            Version:
           </strong>
           0.2.2
          </p>
          <h2 class="text-xl font-bold text-brand-primary pt-8">
           Model README:
          </h2>
          <div class="markdown">
           <h1>
            Description
           </h1>
           <p>
            Detailed whole brain segmentation is an essential quantitative technique in medical image analysis, which provides a non-invasive way of measuring brain regions from a clinical acquired structural magnetic resonance imaging (MRI).
We provide the pre-trained model for training and inferencing whole brain segmentation with 133 structures.
Training pipeline is provided to support active learning in MONAI Label and training with bundle.
           </p>
           <p>
            A tutorial and release of model for whole brain segmentation using the 3D transformer-based segmentation model UNEST.
           </p>
           <p>
            Authors:
Xin Yu (xin.yu@vanderbilt.edu)
           </p>
           <p>
            Yinchi Zhou (yinchi.zhou@vanderbilt.edu) | Yucheng Tang (yuchengt@nvidia.com)
           </p>
           <p align="center">
            -------------------------------------------------------------------------------------
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBrainSeg_Large_UNEST_segmentation/docs/./demo.png"/>
            <br/>
           </p>
           <p align="center">
            Fig.1 - The demonstration of T1w MRI images registered in MNI space and the whole brain segmentation labels with 133 classes
           </p>
           <h1>
            Model Overview
           </h1>
           <p>
            A pre-trained UNEST base model [1] for volumetric (3D) whole brain segmentation with T1w MR images.
To leverage information across embedded sequences, ”shifted window” transformers
are proposed for dense predictions and modeling multi-scale features. However, these
attempts that aim to complicate the self-attention range often yield high computation
complexity and data inefficiency. Inspired by the aggregation function in the nested
ViT, we propose a new design of a 3D U-shape medical segmentation model with
Nested Transformers (UNesT) hierarchically with the 3D block aggregation function,
that learn locality behaviors for small structures or small dataset. This design retains
the original global self-attention mechanism and achieves information communication
across patches by stacking transformer encoders hierarchically.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBrainSeg_Large_UNEST_segmentation/docs/./unest.png"/>
            <br/>
           </p>
           <p align="center">
            Fig.2 - The network architecture of UNEST Base model
           </p>
           <h2>
            Data
           </h2>
           <p>
            The training data is from the Vanderbilt University and Vanderbilt University Medical Center with public released OASIS and CANDI datsets.
Training and testing data are MRI T1-weighted (T1w) 3D volumes coming from 3 different sites. There are a total of 133 classes in the whole brain segmentation task.
Among 50 T1w MRI scans from Open Access Series on Imaging Studies (OASIS) (Marcus et al., 2007) dataset, 45 scans are used for training and the other 5 for validation.
 The testing cohort contains Colin27 T1w scan (Aubert-Broche et al., 2006) and 13 T1w MRI scans from the Child and Adolescent Neuro Development Initiative (CANDI)
 (Kennedy et al., 2012). All data are registered to the MNI space using the MNI305 (Evans et al., 1993) template and preprocessed follow the method in (Huo et al., 2019). Input images are randomly cropped to the size of 96 × 96 × 96.
           </p>
           <h3>
            Important
           </h3>
           <p>
            The brain MRI images for training are registered to Affine registration from the target image to the MNI305 template using NiftyReg.
The data should be in the MNI305 space before inference.
           </p>
           <p>
            If your images are already in MNI space, skip the registration step.
           </p>
           <p>
            You could use any resitration tool to register image to MNI space. Here is an example using ants.
Registration to MNI Space: Sample suggestion. E.g., use ANTS or other tools for registering T1 MRI image to MNI305 Space.
           </p>
           <pre><code>pip install antspyx

#Sample ANTS registration

import ants
import sys
import os

fixed_image = ants.image_read('&lt;fixed_image_path&gt;')
moving_image = ants.image_read('&lt;moving_image_path&gt;')
transform = ants.registration(fixed_image,moving_image,'Affine')

reg3t = ants.apply_transforms(fixed_image,moving_image,transform['fwdtransforms'][0])
ants.image_write(reg3t,output_image_path)
</code></pre>
           <h2>
            Training configuration
           </h2>
           <p>
            The training and inference was performed with at least one 24GB-memory GPU.
           </p>
           <p>
            Actual Model Input: 96 x 96 x 96
           </p>
           <h2>
            Input and output formats
           </h2>
           <p>
            Input: 1 channel T1w MRI image in MNI305 Space.
           </p>
           <h2>
            commands example
           </h2>
           <p>
            Download trained checkpoint model to ./model/model.pt:
           </p>
           <p>
            Add scripts component:  To run the workflow with customized components, PYTHONPATH should be revised to include the path to the customized component:
           </p>
           <pre><code>export PYTHONPATH=$PYTHONPATH: '&lt;path to the bundle root dir&gt;/scripts'
</code></pre>
           <p>
            Execute Training:
           </p>
           <pre><code>python -m monai.bundle run training --meta_file configs/metadata.json --config_file configs/train.json --logging_file configs/logging.conf
</code></pre>
           <p>
            Execute inference:
           </p>
           <pre><code>python -m monai.bundle run evaluating --meta_file configs/metadata.json --config_file configs/inference.json --logging_file configs/logging.conf
</code></pre>
           <h2>
            More examples output
           </h2>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBrainSeg_Large_UNEST_segmentation/docs/./wholebrain.png"/>
            <br/>
           </p>
           <p align="center">
            Fig.3 - The output prediction comparison with variant and ground truth
           </p>
           <h2>
            Training/Validation Benchmarking
           </h2>
           <p>
            A graph showing the training accuracy for fine-tuning 600 epochs.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBrainSeg_Large_UNEST_segmentation/docs/./training.png"/>
            <br/>
           </p>
           <p>
            With 10 fine-tuned labels, the training process converges fast.
           </p>
           <h2>
            Complete ROI of the whole brain segmentation
           </h2>
           <p>
            133 brain structures are segmented.
           </p>
           <table>
            <thead>
             <tr>
              <th style="text-align: left;">
               #1
              </th>
              <th style="text-align: left;">
               #2
              </th>
              <th style="text-align: left;">
               #3
              </th>
              <th style="text-align: left;">
               #4
              </th>
             </tr>
            </thead>
            <tbody>
             <tr>
              <td style="text-align: left;">
               0:  background
              </td>
              <td style="text-align: left;">
               1 :  3rd-Ventricle
              </td>
              <td style="text-align: left;">
               2 :  4th-Ventricle
              </td>
              <td style="text-align: left;">
               3 :  Right-Accumbens-Area
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               4 :  Left-Accumbens-Area
              </td>
              <td style="text-align: left;">
               5 :  Right-Amygdala
              </td>
              <td style="text-align: left;">
               6 :  Left-Amygdala
              </td>
              <td style="text-align: left;">
               7 :  Brain-Stem
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               8 :  Right-Caudate
              </td>
              <td style="text-align: left;">
               9 :  Left-Caudate
              </td>
              <td style="text-align: left;">
               10 :  Right-Cerebellum-Exterior
              </td>
              <td style="text-align: left;">
               11 :  Left-Cerebellum-Exterior
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               12 :  Right-Cerebellum-White-Matter
              </td>
              <td style="text-align: left;">
               13 :  Left-Cerebellum-White-Matter
              </td>
              <td style="text-align: left;">
               14 :  Right-Cerebral-White-Matter
              </td>
              <td style="text-align: left;">
               15 :  Left-Cerebral-White-Matter
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               16 :  Right-Hippocampus
              </td>
              <td style="text-align: left;">
               17 :  Left-Hippocampus
              </td>
              <td style="text-align: left;">
               18 :  Right-Inf-Lat-Vent
              </td>
              <td style="text-align: left;">
               19 :  Left-Inf-Lat-Vent
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               20 :  Right-Lateral-Ventricle
              </td>
              <td style="text-align: left;">
               21 :  Left-Lateral-Ventricle
              </td>
              <td style="text-align: left;">
               22 :  Right-Pallidum
              </td>
              <td style="text-align: left;">
               23 :  Left-Pallidum
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               24 :  Right-Putamen
              </td>
              <td style="text-align: left;">
               25 :  Left-Putamen
              </td>
              <td style="text-align: left;">
               26 :  Right-Thalamus-Proper
              </td>
              <td style="text-align: left;">
               27 :  Left-Thalamus-Proper
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               28 :  Right-Ventral-DC
              </td>
              <td style="text-align: left;">
               29 :  Left-Ventral-DC
              </td>
              <td style="text-align: left;">
               30 :  Cerebellar-Vermal-Lobules-I-V
              </td>
              <td style="text-align: left;">
               31 :  Cerebellar-Vermal-Lobules-VI-VII
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               32 :  Cerebellar-Vermal-Lobules-VIII-X
              </td>
              <td style="text-align: left;">
               33 :  Left-Basal-Forebrain
              </td>
              <td style="text-align: left;">
               34 :  Right-Basal-Forebrain
              </td>
              <td style="text-align: left;">
               35 :  Right-ACgG--anterior-cingulate-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               36 :  Left-ACgG--anterior-cingulate-gyrus
              </td>
              <td style="text-align: left;">
               37 :  Right-AIns--anterior-insula
              </td>
              <td style="text-align: left;">
               38 :  Left-AIns--anterior-insula
              </td>
              <td style="text-align: left;">
               39 :  Right-AOrG--anterior-orbital-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               40 :  Left-AOrG--anterior-orbital-gyrus
              </td>
              <td style="text-align: left;">
               41 :  Right-AnG---angular-gyrus
              </td>
              <td style="text-align: left;">
               42 :  Left-AnG---angular-gyrus
              </td>
              <td style="text-align: left;">
               43 :  Right-Calc--calcarine-cortex
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               44 :  Left-Calc--calcarine-cortex
              </td>
              <td style="text-align: left;">
               45 :  Right-CO----central-operculum
              </td>
              <td style="text-align: left;">
               46 :  Left-CO----central-operculum
              </td>
              <td style="text-align: left;">
               47 :  Right-Cun---cuneus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               48 :  Left-Cun---cuneus
              </td>
              <td style="text-align: left;">
               49 :  Right-Ent---entorhinal-area
              </td>
              <td style="text-align: left;">
               50 :  Left-Ent---entorhinal-area
              </td>
              <td style="text-align: left;">
               51 :  Right-FO----frontal-operculum
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               52 :  Left-FO----frontal-operculum
              </td>
              <td style="text-align: left;">
               53 :  Right-FRP---frontal-pole
              </td>
              <td style="text-align: left;">
               54 :  Left-FRP---frontal-pole
              </td>
              <td style="text-align: left;">
               55 :  Right-FuG---fusiform-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               56 :  Left-FuG---fusiform-gyrus
              </td>
              <td style="text-align: left;">
               57 :  Right-GRe---gyrus-rectus
              </td>
              <td style="text-align: left;">
               58 :  Left-GRe---gyrus-rectus
              </td>
              <td style="text-align: left;">
               59 :  Right-IOG---inferior-occipital-gyrus ,
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               60 :  Left-IOG---inferior-occipital-gyrus
              </td>
              <td style="text-align: left;">
               61 :  Right-ITG---inferior-temporal-gyrus
              </td>
              <td style="text-align: left;">
               62 :  Left-ITG---inferior-temporal-gyrus
              </td>
              <td style="text-align: left;">
               63 :  Right-LiG---lingual-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               64 :  Left-LiG---lingual-gyrus
              </td>
              <td style="text-align: left;">
               65 :  Right-LOrG--lateral-orbital-gyrus
              </td>
              <td style="text-align: left;">
               66 :  Left-LOrG--lateral-orbital-gyrus
              </td>
              <td style="text-align: left;">
               67 :  Right-MCgG--middle-cingulate-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               68 :  Left-MCgG--middle-cingulate-gyrus
              </td>
              <td style="text-align: left;">
               69 :  Right-MFC---medial-frontal-cortex
              </td>
              <td style="text-align: left;">
               70 :  Left-MFC---medial-frontal-cortex
              </td>
              <td style="text-align: left;">
               71 :  Right-MFG---middle-frontal-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               72 :  Left-MFG---middle-frontal-gyrus
              </td>
              <td style="text-align: left;">
               73 :  Right-MOG---middle-occipital-gyrus
              </td>
              <td style="text-align: left;">
               74 :  Left-MOG---middle-occipital-gyrus
              </td>
              <td style="text-align: left;">
               75 :  Right-MOrG--medial-orbital-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               76 :  Left-MOrG--medial-orbital-gyrus
              </td>
              <td style="text-align: left;">
               77 :  Right-MPoG--postcentral-gyrus
              </td>
              <td style="text-align: left;">
               78 :  Left-MPoG--postcentral-gyrus
              </td>
              <td style="text-align: left;">
               79 :  Right-MPrG--precentral-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               80 :  Left-MPrG--precentral-gyrus
              </td>
              <td style="text-align: left;">
               81 :  Right-MSFG--superior-frontal-gyrus
              </td>
              <td style="text-align: left;">
               82 :  Left-MSFG--superior-frontal-gyrus
              </td>
              <td style="text-align: left;">
               83 :  Right-MTG---middle-temporal-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               84 :  Left-MTG---middle-temporal-gyrus
              </td>
              <td style="text-align: left;">
               85 :  Right-OCP---occipital-pole
              </td>
              <td style="text-align: left;">
               86 :  Left-OCP---occipital-pole
              </td>
              <td style="text-align: left;">
               87 :  Right-OFuG--occipital-fusiform-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               88 :  Left-OFuG--occipital-fusiform-gyrus
              </td>
              <td style="text-align: left;">
               89 :  Right-OpIFG-opercular-part-of-the-IFG
              </td>
              <td style="text-align: left;">
               90 :  Left-OpIFG-opercular-part-of-the-IFG
              </td>
              <td style="text-align: left;">
               91 :  Right-OrIFG-orbital-part-of-the-IFG
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               92 :  Left-OrIFG-orbital-part-of-the-IFG
              </td>
              <td style="text-align: left;">
               93 :  Right-PCgG--posterior-cingulate-gyrus
              </td>
              <td style="text-align: left;">
               94 :  Left-PCgG--posterior-cingulate-gyrus
              </td>
              <td style="text-align: left;">
               95 :  Right-PCu---precuneus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               96 :  Left-PCu---precuneus
              </td>
              <td style="text-align: left;">
               97 :  Right-PHG---parahippocampal-gyrus
              </td>
              <td style="text-align: left;">
               98 :  Left-PHG---parahippocampal-gyrus
              </td>
              <td style="text-align: left;">
               99 :  Right-PIns--posterior-insula
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               100 :  Left-PIns--posterior-insula
              </td>
              <td style="text-align: left;">
               101 :  Right-PO----parietal-operculum
              </td>
              <td style="text-align: left;">
               102 :  Left-PO----parietal-operculum
              </td>
              <td style="text-align: left;">
               103 :  Right-PoG---postcentral-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               104 :  Left-PoG---postcentral-gyrus
              </td>
              <td style="text-align: left;">
               105 :  Right-POrG--posterior-orbital-gyrus
              </td>
              <td style="text-align: left;">
               106 :  Left-POrG--posterior-orbital-gyrus
              </td>
              <td style="text-align: left;">
               107 :  Right-PP----planum-polare
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               108 :  Left-PP----planum-polare
              </td>
              <td style="text-align: left;">
               109 :  Right-PrG---precentral-gyrus
              </td>
              <td style="text-align: left;">
               110 :  Left-PrG---precentral-gyrus
              </td>
              <td style="text-align: left;">
               111 :  Right-PT----planum-temporale
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               112 :  Left-PT----planum-temporale
              </td>
              <td style="text-align: left;">
               113 :  Right-SCA---subcallosal-area
              </td>
              <td style="text-align: left;">
               114 :  Left-SCA---subcallosal-area
              </td>
              <td style="text-align: left;">
               115 :  Right-SFG---superior-frontal-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               116 :  Left-SFG---superior-frontal-gyrus
              </td>
              <td style="text-align: left;">
               117 :  Right-SMC---supplementary-motor-cortex
              </td>
              <td style="text-align: left;">
               118 :  Left-SMC---supplementary-motor-cortex
              </td>
              <td style="text-align: left;">
               119 :  Right-SMG---supramarginal-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               120 :  Left-SMG---supramarginal-gyrus
              </td>
              <td style="text-align: left;">
               121 :  Right-SOG---superior-occipital-gyrus
              </td>
              <td style="text-align: left;">
               122 :  Left-SOG---superior-occipital-gyrus
              </td>
              <td style="text-align: left;">
               123 :  Right-SPL---superior-parietal-lobule
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               124 :  Left-SPL---superior-parietal-lobule
              </td>
              <td style="text-align: left;">
               125 :  Right-STG---superior-temporal-gyrus
              </td>
              <td style="text-align: left;">
               126 :  Left-STG---superior-temporal-gyrus
              </td>
              <td style="text-align: left;">
               127 :  Right-TMP---temporal-pole
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               128 :  Left-TMP---temporal-pole
              </td>
              <td style="text-align: left;">
               129 :  Right-TrIFG-triangular-part-of-the-IFG
              </td>
              <td style="text-align: left;">
               130 :  Left-TrIFG-triangular-part-of-the-IFG
              </td>
              <td style="text-align: left;">
               131 :  Right-TTG---transverse-temporal-gyrus
              </td>
             </tr>
             <tr>
              <td style="text-align: left;">
               132 :  Left-TTG---transverse-temporal-gyrus
              </td>
              <td style="text-align: left;">
              </td>
              <td style="text-align: left;">
              </td>
              <td style="text-align: left;">
              </td>
             </tr>
            </tbody>
           </table>
           <h2>
            Bundle Integration in MONAI Lable
           </h2>
           <p>
            The inference and training pipleine can be easily used by the MONAI Label server and 3D Slicer for fast labeling T1w MRI images in MNI space.
           </p>
           <p>
            <img alt="" src="https://raw.githubusercontent.com/Project-MONAI/model-zoo/dev/models/wholeBrainSeg_Large_UNEST_segmentation/docs/./3DSlicer_use.png"/>
            <br/>
           </p>
           <h1>
            Disclaimer
           </h1>
           <p>
            This is an example, not to be used for diagnostic purposes.
           </p>
           <h1>
            References
           </h1>
           <p>
            [1] Yu, Xin, Yinchi Zhou, Yucheng Tang et al.  Characterizing Renal Structures with 3D Block Aggregate Transformers.  arXiv preprint arXiv:2203.02430 (2022). https://arxiv.org/pdf/2203.02430.pdf
           </p>
           <p>
            [2] Zizhao Zhang et al.  Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding.  AAAI Conference on Artificial Intelligence (AAAI) 2022
           </p>
           <p>
            [3] Huo, Yuankai, et al.  3D whole brain segmentation using spatially localized atlas network tiles.  NeuroImage 194 (2019): 105-119.
           </p>
           <h1>
            License
           </h1>
           <p>
            Copyright (c) MONAI Consortium
           </p>
           <p>
            Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
           </p>
           <pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
           <p>
            Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
           </p>
          </div>
         </div>
         <div class="dialog-footer">
          <a class="brand-btn" download="" href="https://github.com/Project-MONAI/model-zoo/releases/download/hosting_storage_v1/wholeBrainSeg_Large_UNEST_segmentation_v0.2.2.zip">
           Download
          </a>
          <button @click="close" class="brand-btn">
           Close
          </button>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </section>
  <!-- Footer Section Start -->
  <footer class="bg-neutral-lightgray py-2 border-t border-neutral-light_gray" id="footer">
   <div class="container">
    <div class="flex flex-wrap items-center justify-between">
     <div class="w-auto">
      <div class="mx-3">
       <div class="footer-logo">
        <a href="index.html">
         <img alt="" src="assets/img/MONAI-logo_color.png"/>
        </a>
       </div>
      </div>
     </div>
     <div class="hidden lg:block w-auto">
      <div class="mx-3 text-black">
       <ul class="flex justify-between">
        <li class="px-4">
         <a class="footer-links" href="about.html">
          About Us
         </a>
        </li>
        <li class="px-4">
         <a class="footer-links" href="started.html">
          Get Started
         </a>
        </li>
        <li class="px-4">
         <a class="footer-links" href="community.html">
          Community
         </a>
        </li>
        <li class="px-4">
         <a class="footer-links" href="https://medium.com/@monai">
          Blog
         </a>
        </li>
        <li class="px-4">
         <a class="footer-links" href="docs.html">
          Docs
         </a>
        </li>
        <li class="px-4">
         <a class="footer-links" href="https://github.com/Project-MONAI">
          GitHub
         </a>
        </li>
       </ul>
      </div>
     </div>
     <div class="w-auto">
      <div class="mx-3">
       <ul class="social-icons flex justify-end">
        <li class="mx-2">
         <a href="https://twitter.com/ProjectMONAI">
          <i>
           <img src="assets/img/twitter_icon_square.png">
           </img>
          </i>
         </a>
        </li>
        <li class="mx-2">
         <a href="https://medium.com/@monai">
          <i>
           <img src="assets/img/medium_icon_square.png">
           </img>
          </i>
         </a>
        </li>
        <li class="mx-2">
         <a href="https://www.youtube.com/c/Project-MONAI">
          <i>
           <img src="assets/img/youtube_icon_square.png">
           </img>
          </i>
         </a>
        </li>
       </ul>
      </div>
     </div>
    </div>
   </div>
  </footer>
  <!-- Footer Section End -->
  <!-- Go to Top Link -->
  <!-- <a href="#" class="back-to-top w-10 h-10 fixed bottom-0 right-0 mb-5 mr-5 flex items-center justify-center rounded-full border border-solid border-brand-dark text-white text-lg z-20">
      <i class="transform -rotate-90">
        <img src="assets/img/ic_arrow_right.png"/>
      </i>
    </a> -->
  <!-- All js Here -->
  <script src="assets/js/wow.js">
  </script>
  <script src="assets/js/main.js">
  </script>
 </body>
</html>